import pandas as pd
import numpy as np
import json
import os
import re
from datetime import datetime
import warnings
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import gc
import requests  # æ·»åŠ requestså¯¼å…¥
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“ - å¯ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import seaborn as sns

# æ·±åº¦å­¦ä¹ åº“ - GPUä¼˜åŒ–
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.parallel import DataParallel
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel, get_scheduler
from transformers import DistilBertTokenizer, DistilBertModel, RobertaTokenizer, RobertaModel
import accelerate
from torch.cuda.amp import autocast, GradScaler

# è®¾ç½®CPUæ ¸å¿ƒæ•°
CPU_CORES = 20  # æ‚¨çš„çº¿ç¨‹æ•°
os.environ["OMP_NUM_THREADS"] = str(CPU_CORES)
os.environ["MKL_NUM_THREADS"] = str(CPU_CORES)

class HighPerformanceMBTITrainer:
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.device = self.setup_gpu()
        self.cpu_cores = CPU_CORES
        
        print(f"ğŸš€ é«˜æ€§èƒ½MBTIè®­ç»ƒç³»ç»Ÿåˆå§‹åŒ–")
        print(f"ğŸ’» CPUæ ¸å¿ƒ: {self.cpu_cores}")
        print(f"ğŸ® GPUè®¾å¤‡: {self.device}")
        print(f"ğŸ”¥ GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
        
        # æ¨¡å‹å­˜å‚¨
        self.models = {}
        self.deep_models = {}
        self.tokenizers = {}
        
        # MBTIç»´åº¦
        self.dimensions = ['E_I', 'S_N', 'T_F', 'J_P']
        self.dim_names = {
            'E_I': 'Extraversion vs Introversion',
            'S_N': 'Sensing vs Intuition', 
            'T_F': 'Thinking vs Feeling',
            'J_P': 'Judging vs Perceiving'
        }
    
    def setup_gpu(self):
        """è®¾ç½®GPUä¼˜åŒ–"""
        if torch.cuda.is_available():
            device = torch.device('cuda')
            print(f"ğŸ® æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
            
            # ä¼˜åŒ–GPUè®¾ç½®
            torch.backends.cudnn.benchmark = True  # ä¼˜åŒ–cuDNNæ€§èƒ½
            torch.backends.cudnn.deterministic = False  # å…è®¸éç¡®å®šæ€§æ“ä½œä»¥æé«˜æ€§èƒ½
            
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
            
            return device
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")
            return torch.device('cpu')
    
    def load_data_optimized(self):
        """ä¼˜åŒ–çš„æ•°æ®åŠ è½½"""
        print("\nğŸ“Š åŠ è½½æ•°æ®ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰...")
        
        # å¹¶è¡ŒåŠ è½½æ–‡ä»¶
        files = {
            'train': 'enhanced_english_mbti_train.csv',
            'val': 'enhanced_english_mbti_val.csv', 
            'test': 'enhanced_english_mbti_test.csv',
            'keywords': 'english_keywords.json'
        }
        
        def load_file(file_info):
            name, filename = file_info
            filepath = os.path.join(self.data_dir, filename)
            
            if filename.endswith('.csv'):
                # ä¼˜åŒ–pandasè¯»å–
                df = pd.read_csv(filepath, engine='c')  # ä½¿ç”¨Cå¼•æ“åŠ é€Ÿ
                return name, df
            elif filename.endswith('.json'):
                with open(filepath, 'r', encoding='utf-8') as f:
                    return name, json.load(f)
        
        # å¹¶è¡ŒåŠ è½½
        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(load_file, files.items()))
        
        # åˆ†é…æ•°æ®
        for name, data in results:
            if name == 'train':
                self.train_df = data
            elif name == 'val':
                self.val_df = data
            elif name == 'test':
                self.test_df = data
            elif name == 'keywords':
                self.keywords = data
        
        print(f"âœ… è®­ç»ƒé›†: {len(self.train_df):,} æ ·æœ¬")
        print(f"âœ… éªŒè¯é›†: {len(self.val_df):,} æ ·æœ¬") 
        print(f"âœ… æµ‹è¯•é›†: {len(self.test_df):,} æ ·æœ¬")
        
        # åˆ›å»ºMBTIç»´åº¦æ ‡ç­¾
        self.create_labels()
        
        # é¢„å¤„ç†æ–‡æœ¬åˆ°å†…å­˜
        self.preprocess_all_texts()
        
        return self.train_df, self.val_df, self.test_df
    
    def create_labels(self):
        """åˆ›å»ºæ‰€æœ‰ç»´åº¦æ ‡ç­¾"""
        for df in [self.train_df, self.val_df, self.test_df]:
            df['E_I'] = (df['type'].str[0] == 'E').astype(int)
            df['S_N'] = (df['type'].str[1] == 'S').astype(int)
            df['T_F'] = (df['type'].str[2] == 'T').astype(int)
            df['J_P'] = (df['type'].str[3] == 'J').astype(int)
    
    def preprocess_batch(self, texts):
        """é¢„å¤„ç†ä¸€æ‰¹æ–‡æœ¬"""
        return [self.preprocess_text(text) for text in texts]
    
    def preprocess_all_texts(self):
        """é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬åˆ°å†…å­˜"""
        print("ğŸ”„ é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬...")
        
        # å¹¶è¡Œé¢„å¤„ç† - ä½¿ç”¨ThreadPoolExecutoré¿å…pickleé—®é¢˜
        batch_size = 1000
        for df_name, df in [('train', self.train_df), ('val', self.val_df), ('test', self.test_df)]:
            texts = df['normalized_posts'].fillna('').tolist()
            processed_texts = []
            
            # åˆ†æ‰¹å¹¶è¡Œå¤„ç† - æ”¹ç”¨ThreadPoolExecutor
            with ThreadPoolExecutor(max_workers=min(self.cpu_cores, 8)) as executor:
                batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]
                results = list(executor.map(self.preprocess_batch, batches))
                
                for batch_result in results:
                    processed_texts.extend(batch_result)
            
            df['processed_text'] = processed_texts
            print(f"âœ… {df_name} æ–‡æœ¬é¢„å¤„ç†å®Œæˆ")
    
    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†"""
        if pd.isna(text):
            return ""
        
        text = str(text).lower()
        
        # é«˜æ•ˆçš„æ–‡æœ¬æ¸…ç†
        replacements = {
            r"won't": "will not", r"can't": "cannot", r"n't": " not",
            r"i'm": "i am", r"you're": "you are", r"it's": "it is"
        }
        
        for pattern, replacement in replacements.items():
            text = re.sub(pattern, replacement, text)
        
        # æ¸…ç†URLå’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'http[s]?://\S+', ' ', text)
        text = re.sub(r'[^\w\s\.\!\?\,]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def create_optimized_features(self):
        """åˆ›å»ºä¼˜åŒ–çš„ç‰¹å¾"""
        print("\nğŸ§  ç‰¹å¾å·¥ç¨‹ï¼ˆå¤šæ ¸å¹¶è¡Œï¼‰...")
        
        # 1. é«˜æ€§èƒ½TF-IDFç‰¹å¾
        print("ğŸ“ TF-IDFç‰¹å¾æå–...")
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=10000,  # å¢åŠ ç‰¹å¾æ•°é‡
            stop_words='english',
            ngram_range=(1, 3),  # å¢åŠ åˆ°3-gram
            min_df=2,
            max_df=0.9,
            sublinear_tf=True,  # ä½¿ç”¨sublinear TF scaling
            norm='l2'
        )
        
        # å¹¶è¡ŒTF-IDF
        train_texts = self.train_df['processed_text'].tolist()
        val_texts = self.val_df['processed_text'].tolist()
        test_texts = self.test_df['processed_text'].tolist()
        
        self.X_train_tfidf = self.tfidf_vectorizer.fit_transform(train_texts)
        self.X_val_tfidf = self.tfidf_vectorizer.transform(val_texts)
        self.X_test_tfidf = self.tfidf_vectorizer.transform(test_texts)
        
        print(f"âœ… TF-IDFç‰¹å¾: {self.X_train_tfidf.shape[1]:,} ç»´")
        
        # 2. å¹¶è¡Œå…³é”®è¯ç‰¹å¾
        print("ğŸ”‘ å…³é”®è¯ç‰¹å¾æå–...")
        with ThreadPoolExecutor(max_workers=min(self.cpu_cores, 8)) as executor:
            keyword_futures = [
                executor.submit(self.create_keyword_features, df, 'processed_text')
                for df in [self.train_df, self.val_df, self.test_df]
            ]
            
            keyword_results = [future.result() for future in keyword_futures]
        
        self.X_train_keywords, self.X_val_keywords, self.X_test_keywords = keyword_results
        print(f"âœ… å…³é”®è¯ç‰¹å¾: {self.X_train_keywords.shape[1]} ç»´")
        
        # 3. å¹¶è¡Œç»Ÿè®¡ç‰¹å¾
        print("ğŸ“Š ç»Ÿè®¡ç‰¹å¾æå–...")
        with ThreadPoolExecutor(max_workers=min(self.cpu_cores, 8)) as executor:
            stats_futures = [
                executor.submit(self.create_advanced_stats_features, df, 'processed_text')
                for df in [self.train_df, self.val_df, self.test_df]
            ]
            
            stats_results = [future.result() for future in stats_futures]
        
        self.X_train_stats, self.X_val_stats, self.X_test_stats = stats_results
        print(f"âœ… ç»Ÿè®¡ç‰¹å¾: {self.X_train_stats.shape[1]} ç»´")
        
        # 4. ç»„åˆç‰¹å¾
        from scipy import sparse
        
        self.X_train = sparse.hstack([
            self.X_train_tfidf, 
            self.X_train_keywords, 
            self.X_train_stats
        ])
        self.X_val = sparse.hstack([
            self.X_val_tfidf, 
            self.X_val_keywords, 
            self.X_val_stats
        ])
        self.X_test = sparse.hstack([
            self.X_test_tfidf, 
            self.X_test_keywords, 
            self.X_test_stats
        ])
        
        print(f"ğŸ¯ æ€»ç‰¹å¾ç»´åº¦: {self.X_train.shape[1]:,}")
        
        return self.X_train, self.X_val, self.X_test
    
    def create_keyword_features(self, df, text_column):
        """åˆ›å»ºå…³é”®è¯ç‰¹å¾"""
        features = []
        
        for _, row in df.iterrows():
            text = str(row[text_column]).lower()
            feature_vector = []
            
            for dim in ['E', 'S', 'T', 'J']:
                if dim in self.keywords:
                    keywords_data = self.keywords[dim][:30]  # ä½¿ç”¨æ›´å¤šå…³é”®è¯
                    
                    pos_score = 0
                    neg_score = 0
                    weighted_score = 0
                    
                    for kw_info in keywords_data:
                        keyword = kw_info['keyword']
                        score = kw_info.get('score', 0)
                        importance = kw_info.get('abs_score', abs(score))
                        
                        count = text.count(keyword)
                        if count > 0:
                            if score > 0:
                                pos_score += count * importance
                            else:
                                neg_score += count * importance
                            weighted_score += count * score
                    
                    feature_vector.extend([pos_score, neg_score, weighted_score, pos_score - neg_score])
                else:
                    feature_vector.extend([0, 0, 0, 0])
            
            features.append(feature_vector)
        
        return np.array(features)
    
    def create_advanced_stats_features(self, df, text_column):
        """åˆ›å»ºé«˜çº§ç»Ÿè®¡ç‰¹å¾"""
        features = []
        
        for _, row in df.iterrows():
            text = str(row[text_column])
            words = text.split()
            sentences = text.split('.')
            
            # åŸºç¡€ç»Ÿè®¡
            text_len = len(text)
            word_count = len(words)
            sent_count = len([s for s in sentences if s.strip()])
            
            # é«˜çº§ç‰¹å¾
            avg_word_len = np.mean([len(w) for w in words]) if words else 0
            vocab_diversity = len(set(words)) / len(words) if words else 0
            
            # æ ‡ç‚¹å’Œè¯­è°ƒ
            exclamation_ratio = text.count('!') / text_len if text_len > 0 else 0
            question_ratio = text.count('?') / text_len if text_len > 0 else 0
            caps_ratio = sum(1 for c in text if c.isupper()) / text_len if text_len > 0 else 0
            
            # äººç§°ä»£è¯åˆ†æ
            first_person = (text.count(' i ') + text.count('me') + text.count('my')) / word_count if word_count > 0 else 0
            second_person = (text.count('you') + text.count('your')) / word_count if word_count > 0 else 0
            third_person = (text.count('he') + text.count('she') + text.count('they')) / word_count if word_count > 0 else 0
            
            # æƒ…æ„Ÿè¯æ±‡
            positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'love', 'like', 'enjoy']
            negative_words = ['bad', 'terrible', 'awful', 'hate', 'dislike', 'horrible', 'worst', 'sad', 'angry']
            
            pos_sentiment = sum(text.count(word) for word in positive_words) / word_count if word_count > 0 else 0
            neg_sentiment = sum(text.count(word) for word in negative_words) / word_count if word_count > 0 else 0
            
            # å¤æ‚åº¦æŒ‡æ ‡
            avg_sent_len = word_count / sent_count if sent_count > 0 else 0
            long_words_ratio = len([w for w in words if len(w) > 6]) / word_count if word_count > 0 else 0
            
            stats = [
                text_len, word_count, sent_count, avg_word_len, vocab_diversity,
                exclamation_ratio, question_ratio, caps_ratio,
                first_person, second_person, third_person,
                pos_sentiment, neg_sentiment, avg_sent_len, long_words_ratio
            ]
            
            features.append(stats)
        
        return np.array(features)
    
    def train_model_dimension(self, args):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹çš„å•ä¸ªç»´åº¦"""
        model_name, model, dim_name = args
        
        y_train = self.train_df[dim_name].values
        y_val = self.val_df[dim_name].values
        
        # è®­ç»ƒæ¨¡å‹
        if model_name == 'SVM':
            # SVMä½¿ç”¨è¾ƒå°‘çš„ç‰¹å¾ä»¥æé«˜é€Ÿåº¦
            from sklearn.feature_selection import SelectKBest, f_classif
            selector = SelectKBest(f_classif, k=min(5000, self.X_train.shape[1]))
            X_train_selected = selector.fit_transform(self.X_train, y_train)
            X_val_selected = selector.transform(self.X_val)
            
            model.fit(X_train_selected, y_train)
            y_pred = model.predict(X_val_selected)
            y_prob = model.predict_proba(X_val_selected)[:, 1]
        else:
            model.fit(self.X_train, y_train)
            y_pred = model.predict(self.X_val)
            y_prob = model.predict_proba(self.X_val)[:, 1]
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y_val, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')
        auc = roc_auc_score(y_val, y_prob)
        
        return dim_name, model_name, {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'model': model
        }
    
    def train_traditional_models_parallel(self):
        """å¹¶è¡Œè®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹"""
        print("\nğŸ¤– å¹¶è¡Œè®­ç»ƒä¼ ç»Ÿæ¨¡å‹...")
        
        # é«˜æ€§èƒ½æ¨¡å‹é…ç½®
        models = {
            'RandomForest': RandomForestClassifier(
                n_estimators=200,  # å¢åŠ æ ‘çš„æ•°é‡
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                n_jobs=self.cpu_cores,  # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
                random_state=42
            ),
            'ExtraTrees': ExtraTreesClassifier(
                n_estimators=200,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                n_jobs=self.cpu_cores,
                random_state=42
            ),
            'LogisticRegression': LogisticRegression(
                random_state=42,
                max_iter=2000,
                n_jobs=self.cpu_cores,
                solver='saga'  # æ”¯æŒå¹¶è¡Œçš„æ±‚è§£å™¨
            ),
            'SVM': SVC(
                random_state=42,
                probability=True,
                kernel='rbf',
                cache_size=2000  # å¢åŠ ç¼“å­˜
            )
        }
        
        self.traditional_results = {}
        
        # ç”±äºpickleé—®é¢˜ï¼Œæ”¹ä¸ºä¸²è¡Œè®­ç»ƒä½†æ¯ä¸ªæ¨¡å‹å†…éƒ¨å¹¶è¡Œ
        print(f"ğŸš€ å¯åŠ¨ä¼ ç»Ÿæ¨¡å‹è®­ç»ƒï¼ˆæ¯ä¸ªæ¨¡å‹å†…éƒ¨å¹¶è¡Œï¼‰...")
        
        for dim_name in self.dimensions:
            self.traditional_results[dim_name] = {}
            
            for model_name, model in models.items():
                y_train = self.train_df[dim_name].values
                y_val = self.val_df[dim_name].values
                
                # è®­ç»ƒæ¨¡å‹
                if model_name == 'SVM':
                    # SVMä½¿ç”¨è¾ƒå°‘çš„ç‰¹å¾ä»¥æé«˜é€Ÿåº¦
                    from sklearn.feature_selection import SelectKBest, f_classif
                    selector = SelectKBest(f_classif, k=min(5000, self.X_train.shape[1]))
                    X_train_selected = selector.fit_transform(self.X_train, y_train)
                    X_val_selected = selector.transform(self.X_val)
                    
                    model.fit(X_train_selected, y_train)
                    y_pred = model.predict(X_val_selected)
                    y_prob = model.predict_proba(X_val_selected)[:, 1]
                else:
                    model.fit(self.X_train, y_train)
                    y_pred = model.predict(self.X_val)
                    y_prob = model.predict_proba(self.X_val)[:, 1]
                
                # è®¡ç®—æŒ‡æ ‡
                accuracy = accuracy_score(y_val, y_pred)
                precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')
                auc = roc_auc_score(y_val, y_prob)
                
                self.traditional_results[dim_name][model_name] = {
                    'accuracy': accuracy,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'auc': auc,
                    'model': model
                }
                
                print(f"âœ… {dim_name}-{model_name}: F1={f1:.3f}, AUC={auc:.3f}")
        
        return self.traditional_results
    
    def create_gpu_ensemble_models(self):
        """åˆ›å»ºGPUåŠ é€Ÿçš„é›†æˆæ¨¡å‹"""
        print("\nğŸ”¥ åˆ›å»ºGPUé›†æˆæ¨¡å‹...")
        
        self.ensemble_results = {}
        
        for dim_name in self.dimensions:
            print(f"ğŸ¯ å¤„ç† {dim_name} ç»´åº¦...")
            
            # é€‰æ‹©æœ€ä½³çš„3ä¸ªæ¨¡å‹
            dim_results = self.traditional_results[dim_name]
            sorted_models = sorted(dim_results.items(), key=lambda x: x[1]['f1'], reverse=True)
            top_models = sorted_models[:3]
            
            # åˆ›å»ºåŠ æƒæŠ•ç¥¨é›†æˆ
            estimators = [(name, result['model']) for name, result in top_models]
            weights = [result['f1'] for _, result in top_models]  # åŸºäºF1åˆ†æ•°çš„æƒé‡
            
            ensemble = VotingClassifier(
                estimators=estimators,
                voting='soft',
                weights=weights,
                n_jobs=self.cpu_cores
            )
            
            # è®­ç»ƒé›†æˆæ¨¡å‹
            y_train = self.train_df[dim_name].values
            y_val = self.val_df[dim_name].values
            
            ensemble.fit(self.X_train, y_train)
            y_pred = ensemble.predict(self.X_val)
            y_prob = ensemble.predict_proba(self.X_val)[:, 1]
            
            # è¯„ä¼°
            accuracy = accuracy_score(y_val, y_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='binary')
            auc = roc_auc_score(y_val, y_prob)
            
            self.ensemble_results[dim_name] = {
                'model': ensemble,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc,
                'base_models': [name for name, _ in top_models],
                'weights': weights
            }
            
            print(f"  ğŸ‰ é›†æˆæ¨¡å‹ F1: {f1:.3f}, AUC: {auc:.3f}")
            print(f"  ğŸ“Š ä½¿ç”¨æ¨¡å‹: {', '.join([name for name, _ in top_models])}")
        
        return self.ensemble_results

# PyTorchæ•°æ®é›†ç±»
class MBTIDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        labels = torch.tensor(self.labels[idx], dtype=torch.float32)
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': labels
        }

# GPUä¼˜åŒ–çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
class MBTITransformerModel(nn.Module):
    def __init__(self, model_name='distilbert-base-uncased', num_labels=4):
        super().__init__()
        
        if 'distilbert' in model_name:
            self.bert = DistilBertModel.from_pretrained(model_name)
            hidden_size = self.bert.config.hidden_size
        elif 'roberta' in model_name:
            self.bert = RobertaModel.from_pretrained(model_name)
            hidden_size = self.bert.config.hidden_size
        else:
            self.bert = AutoModel.from_pretrained(model_name)
            hidden_size = self.bert.config.hidden_size
        
        # å¤šå±‚åˆ†ç±»å¤´
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_labels)
        )
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token
        output = self.dropout(pooled_output)
        return self.classifier(output)

class HighPerformanceMBTITrainer(HighPerformanceMBTITrainer):
    def check_network_and_models(self):
        """æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ¨¡å‹å¯ç”¨æ€§"""
        print("\nğŸ” æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ¨¡å‹å¯ç”¨æ€§...")
        
        try:
            import requests
            response = requests.get("https://huggingface.co", timeout=10)
            if response.status_code == 200:
                print("âœ… ç½‘ç»œè¿æ¥æ­£å¸¸")
                return True
        except Exception as e:
            print(f"âŒ ç½‘ç»œè¿æ¥å¼‚å¸¸: {e}")
            print("âš ï¸ å°†è·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒï¼Œä»…ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹")
            return False
    
    def train_gpu_deep_models(self):
        """è®­ç»ƒGPUæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        print("\nğŸš€ è®­ç»ƒGPUæ·±åº¦å­¦ä¹ æ¨¡å‹...")
        
        # æ£€æŸ¥ç½‘ç»œè¿æ¥
        if not self.check_network_and_models():
            print("âš ï¸ ç”±äºç½‘ç»œé—®é¢˜ï¼Œè·³è¿‡æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒ")
            self.deep_results = {}
            return self.deep_results
        
        # æ¨¡å‹é…ç½®
        model_configs = [
            ('distilbert-base-uncased', 64, 3e-5),  # DistilBERT - å¿«é€Ÿ
            ('roberta-base', 32, 2e-5),             # RoBERTa - å‡†ç¡®
        ]
        
        self.deep_results = {}
        
        for model_name, batch_size, learning_rate in model_configs:
            print(f"\nğŸ¤– è®­ç»ƒ {model_name}...")
            
            try:
                # åˆ›å»ºtokenizer
                if 'distilbert' in model_name:
                    tokenizer = DistilBertTokenizer.from_pretrained(model_name)
                elif 'roberta' in model_name:
                    tokenizer = RobertaTokenizer.from_pretrained(model_name)
                else:
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                
                print(f"âœ… æˆåŠŸåŠ è½½ {model_name} tokenizer")
                
            except Exception as e:
                print(f"âŒ æ— æ³•åŠ è½½ {model_name}: {e}")
                print(f"âš ï¸ è·³è¿‡ {model_name} æ¨¡å‹è®­ç»ƒ")
                continue
            
            try:
                # å‡†å¤‡æ•°æ®
                train_labels = self.train_df[self.dimensions].values
                val_labels = self.val_df[self.dimensions].values
                
                train_dataset = MBTIDataset(
                    self.train_df['processed_text'].tolist(),
                    train_labels,
                    tokenizer,
                    max_length=256  # ä¼˜åŒ–åºåˆ—é•¿åº¦ä»¥æé«˜é€Ÿåº¦
                )
                
                val_dataset = MBTIDataset(
                    self.val_df['processed_text'].tolist(),
                    val_labels,
                    tokenizer,
                    max_length=256
                )
                
                # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ä¼˜åŒ–æ‰¹é‡å¤§å°ä»¥å……åˆ†åˆ©ç”¨5090
                train_loader = DataLoader(
                    train_dataset,
                    batch_size=batch_size,
                    shuffle=True,
                    num_workers=4,  # å‡å°‘workeræ•°é‡é¿å…æ½œåœ¨é—®é¢˜
                    pin_memory=True if torch.cuda.is_available() else False,
                    persistent_workers=False  # é¿å…ä¸€äº›Windowså…¼å®¹æ€§é—®é¢˜
                )
                
                val_loader = DataLoader(
                    val_dataset,
                    batch_size=batch_size * 2,  # éªŒè¯æ—¶å¯ä»¥ç”¨æ›´å¤§æ‰¹é‡
                    shuffle=False,
                    num_workers=4,
                    pin_memory=True if torch.cuda.is_available() else False,
                    persistent_workers=False
                )
                
                # åˆ›å»ºæ¨¡å‹
                model = MBTITransformerModel(model_name, num_labels=4)
                
                # ä½¿ç”¨DataParallelå¦‚æœæœ‰å¤šGPUï¼ˆ5090æ˜¯å•GPUä½†å¾ˆå¼ºå¤§ï¼‰
                if torch.cuda.device_count() > 1:
                    model = DataParallel(model)
                
                model = model.to(self.device)
                
                # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
                optimizer = optim.AdamW(
                    model.parameters(),
                    lr=learning_rate,
                    weight_decay=0.01
                )
                
                num_epochs = 3  # å‡å°‘epochsé¿å…é•¿æ—¶é—´è®­ç»ƒ
                num_training_steps = len(train_loader) * num_epochs
                scheduler = get_scheduler(
                    "linear",
                    optimizer=optimizer,
                    num_warmup_steps=int(0.1 * num_training_steps),
                    num_training_steps=num_training_steps
                )
                
                # æ··åˆç²¾åº¦è®­ç»ƒ - å……åˆ†åˆ©ç”¨5090çš„Tensor Cores
                scaler = GradScaler() if torch.cuda.is_available() else None
                
                # è®­ç»ƒå¾ªç¯
                model.train()
                for epoch in range(num_epochs):
                    print(f"  ğŸ“… Epoch {epoch+1}/{num_epochs}")
                    
                    total_loss = 0
                    for batch_idx, batch in enumerate(train_loader):
                        input_ids = batch['input_ids'].to(self.device)
                        attention_mask = batch['attention_mask'].to(self.device)
                        labels = batch['labels'].to(self.device)
                        
                        optimizer.zero_grad()
                        
                        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                        if scaler and torch.cuda.is_available():
                            with autocast():
                                outputs = model(input_ids, attention_mask)
                                loss = F.binary_cross_entropy_with_logits(outputs, labels)
                            
                            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                            scaler.scale(loss).backward()
                            scaler.step(optimizer)
                            scaler.update()
                        else:
                            outputs = model(input_ids, attention_mask)
                            loss = F.binary_cross_entropy_with_logits(outputs, labels)
                            loss.backward()
                            optimizer.step()
                        
                        scheduler.step()
                        total_loss += loss.item()
                        
                        if batch_idx % 100 == 0:
                            print(f"    ğŸ“ˆ Batch {batch_idx}, Loss: {loss.item():.4f}")
                    
                    avg_loss = total_loss / len(train_loader)
                    print(f"  ğŸ“Š Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
                
                # éªŒè¯
                model.eval()
                val_predictions = []
                val_probabilities = []
                val_true_labels = []
                
                with torch.no_grad():
                    for batch in val_loader:
                        input_ids = batch['input_ids'].to(self.device)
                        attention_mask = batch['attention_mask'].to(self.device)
                        labels = batch['labels'].to(self.device)
                        
                        if scaler and torch.cuda.is_available():
                            with autocast():
                                outputs = model(input_ids, attention_mask)
                        else:
                            outputs = model(input_ids, attention_mask)
                        
                        probabilities = torch.sigmoid(outputs)
                        predictions = (probabilities > 0.5).float()
                        
                        val_predictions.append(predictions.cpu())
                        val_probabilities.append(probabilities.cpu())
                        val_true_labels.append(labels.cpu())
                
                # åˆå¹¶é¢„æµ‹ç»“æœ
                val_predictions = torch.cat(val_predictions, dim=0).numpy()
                val_probabilities = torch.cat(val_probabilities, dim=0).numpy()
                val_true_labels = torch.cat(val_true_labels, dim=0).numpy()
                
                # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æ€§èƒ½
                model_results = {}
                for i, dim_name in enumerate(self.dimensions):
                    y_true = val_true_labels[:, i]
                    y_pred = val_predictions[:, i]
                    y_prob = val_probabilities[:, i]
                    
                    accuracy = accuracy_score(y_true, y_pred)
                    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
                    auc = roc_auc_score(y_true, y_prob)
                    
                    model_results[dim_name] = {
                        'accuracy': accuracy,
                        'precision': precision,
                        'recall': recall,
                        'f1': f1,
                        'auc': auc
                    }
                    
                    print(f"    âœ… {dim_name}: F1={f1:.3f}, AUC={auc:.3f}")
                
                self.deep_results[model_name] = {
                    'model': model,
                    'tokenizer': tokenizer,
                    'results': model_results
                }
                
                print(f"âœ… {model_name} è®­ç»ƒå®Œæˆ")
                
            except Exception as e:
                print(f"âŒ {model_name} è®­ç»ƒå¤±è´¥: {e}")
                continue
            
            finally:
                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
        
        if not self.deep_results:
            print("âš ï¸ æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹éƒ½æ— æ³•è®­ç»ƒï¼Œå°†ä»…ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹")
        
        return self.deep_results
    
    def evaluate_all_models_on_test(self):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        print("\nğŸ¯ æµ‹è¯•é›†æœ€ç»ˆè¯„ä¼°...")
        
        self.final_test_results = {
            'traditional_ensemble': {},
            'deep_learning': {}
        }
        
        # 1. è¯„ä¼°ä¼ ç»Ÿé›†æˆæ¨¡å‹
        print("ğŸ“Š è¯„ä¼°ä¼ ç»Ÿé›†æˆæ¨¡å‹...")
        for dim_name in self.dimensions:
            model = self.ensemble_results[dim_name]['model']
            y_test = self.test_df[dim_name].values
            
            y_pred = model.predict(self.X_test)
            y_prob = model.predict_proba(self.X_test)[:, 1]
            
            accuracy = accuracy_score(y_test, y_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')
            auc = roc_auc_score(y_test, y_prob)
            
            self.final_test_results['traditional_ensemble'][dim_name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc
            }
            
            print(f"  {dim_name}: F1={f1:.3f}, AUC={auc:.3f}")
        
        # 2. è¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.deep_results:
            print("ğŸ¤– è¯„ä¼°æ·±åº¦å­¦ä¹ æ¨¡å‹...")
            for model_name, model_data in self.deep_results.items():
                print(f"  {model_name}:")
                
                try:
                    model = model_data['model']
                    tokenizer = model_data['tokenizer']
                    
                    # å‡†å¤‡æµ‹è¯•æ•°æ®
                    test_labels = self.test_df[self.dimensions].values
                    test_dataset = MBTIDataset(
                        self.test_df['processed_text'].tolist(),
                        test_labels,
                        tokenizer,
                        max_length=256
                    )
                    
                    test_loader = DataLoader(
                        test_dataset,
                        batch_size=64,
                        shuffle=False,
                        num_workers=4,
                        pin_memory=True if torch.cuda.is_available() else False,
                        persistent_workers=False
                    )
                    
                    # é¢„æµ‹
                    model.eval()
                    test_predictions = []
                    test_probabilities = []
                    test_true_labels = []
                    
                    with torch.no_grad():
                        for batch in test_loader:
                            input_ids = batch['input_ids'].to(self.device)
                            attention_mask = batch['attention_mask'].to(self.device)
                            labels = batch['labels'].to(self.device)
                            
                            if torch.cuda.is_available():
                                with autocast():
                                    outputs = model(input_ids, attention_mask)
                            else:
                                outputs = model(input_ids, attention_mask)
                            
                            probabilities = torch.sigmoid(outputs)
                            predictions = (probabilities > 0.5).float()
                            
                            test_predictions.append(predictions.cpu())
                            test_probabilities.append(probabilities.cpu())
                            test_true_labels.append(labels.cpu())
                    
                    # åˆå¹¶ç»“æœ
                    test_predictions = torch.cat(test_predictions, dim=0).numpy()
                    test_probabilities = torch.cat(test_probabilities, dim=0).numpy()
                    test_true_labels = torch.cat(test_true_labels, dim=0).numpy()
                    
                    # è®¡ç®—æ€§èƒ½
                    model_test_results = {}
                    for i, dim_name in enumerate(self.dimensions):
                        y_true = test_true_labels[:, i]
                        y_pred = test_predictions[:, i]
                        y_prob = test_probabilities[:, i]
                        
                        accuracy = accuracy_score(y_true, y_pred)
                        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
                        auc = roc_auc_score(y_true, y_prob)
                        
                        model_test_results[dim_name] = {
                            'accuracy': accuracy,
                            'precision': precision,
                            'recall': recall,
                            'f1': f1,
                            'auc': auc
                        }
                        
                        print(f"    {dim_name}: F1={f1:.3f}, AUC={auc:.3f}")
                    
                    self.final_test_results['deep_learning'][model_name] = model_test_results
                    
                except Exception as e:
                    print(f"    âŒ {model_name} è¯„ä¼°å¤±è´¥: {e}")
                    continue
        else:
            print("âš ï¸ æ²¡æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹éœ€è¦è¯„ä¼°")
        
        return self.final_test_results
    
    def save_all_models_optimized(self):
        """ä¼˜åŒ–çš„æ¨¡å‹ä¿å­˜"""
        print("\nğŸ’¾ ä¿å­˜æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹...")
        
        # ä¿®æ”¹ä¿å­˜è·¯å¾„ä¸ºç”¨æˆ·æŒ‡å®šè·¯å¾„
        models_dir = r"C:\Users\lnasl\Desktop\DeepMBTI\TrainedModel\text\new"
        os.makedirs(models_dir, exist_ok=True)
        
        import joblib
        
        # 1. ä¿å­˜ä¼ ç»Ÿæ¨¡å‹
        traditional_dir = os.path.join(models_dir, 'traditional')
        os.makedirs(traditional_dir, exist_ok=True)
        
        for dim_name in self.dimensions:
            model_file = os.path.join(traditional_dir, f'mbti_{dim_name}_ensemble.pkl')
            joblib.dump(self.ensemble_results[dim_name]['model'], model_file)
            print(f"  âœ… ä¼ ç»Ÿæ¨¡å‹ {dim_name}: {model_file}")
        
        # 2. ä¿å­˜æ·±åº¦å­¦ä¹ æ¨¡å‹
        deep_dir = os.path.join(models_dir, 'deep_learning')
        os.makedirs(deep_dir, exist_ok=True)
        
        for model_name, model_data in self.deep_results.items():
            model_path = os.path.join(deep_dir, f'{model_name.replace("/", "_")}')
            os.makedirs(model_path, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹
            torch.save(model_data['model'].state_dict(), os.path.join(model_path, 'model.pt'))
            
            # ä¿å­˜tokenizer
            model_data['tokenizer'].save_pretrained(model_path)
            
            print(f"  âœ… æ·±åº¦æ¨¡å‹ {model_name}: {model_path}")
        
        # 3. ä¿å­˜ç‰¹å¾æå–å™¨å’Œé…ç½®
        joblib.dump(self.tfidf_vectorizer, os.path.join(models_dir, 'tfidf_vectorizer.pkl'))
        
        config = {
            'keywords': self.keywords,
            'dimensions': self.dimensions,
            'dim_names': self.dim_names,
            'final_test_results': self.final_test_results,
            'training_config': {
                'cpu_cores': self.cpu_cores,
                'gpu_device': str(self.device),
                'training_date': datetime.now().isoformat()
            }
        }
        
        with open(os.path.join(models_dir, 'training_config.json'), 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ‰ æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ°: {models_dir}")
    
    def run_high_performance_training(self):
        """è¿è¡Œé«˜æ€§èƒ½è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¯åŠ¨é«˜æ€§èƒ½MBTIæ¨¡å‹è®­ç»ƒ...")
        print(f"ğŸ’ª ç¡¬ä»¶é…ç½®: {self.cpu_cores}æ ¸å¿ƒCPU + {torch.cuda.get_device_name(0)}")
        
        start_time = datetime.now()
        
        # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
        self.load_data_optimized()
        
        # 2. ç‰¹å¾å·¥ç¨‹
        self.create_optimized_features()
        
        # 3. å¹¶è¡Œè®­ç»ƒä¼ ç»Ÿæ¨¡å‹
        self.train_traditional_models_parallel()
        
        # 4. åˆ›å»ºé›†æˆæ¨¡å‹
        self.create_gpu_ensemble_models()
        
        # 5. è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
        self.train_gpu_deep_models()
        
        # 6. æµ‹è¯•é›†è¯„ä¼°
        self.evaluate_all_models_on_test()
        
        # 7. ä¿å­˜æ‰€æœ‰æ¨¡å‹
        self.save_all_models_optimized()
        
        end_time = datetime.now()
        training_time = end_time - start_time
        
        print(f"\nğŸ‰ é«˜æ€§èƒ½è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {training_time}")
        print(f"ğŸš€ é€Ÿåº¦æå‡: åˆ©ç”¨{self.cpu_cores}æ ¸å¿ƒå¹¶è¡Œ + RTX 5090 GPUåŠ é€Ÿ")
        
        # æ˜¾ç¤ºæœ€ä½³æ€§èƒ½
        print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½æ€»ç»“:")
        print(f"{'='*60}")
        
        # ä¼ ç»Ÿé›†æˆæ¨¡å‹æ€§èƒ½
        print("ğŸ¤– ä¼ ç»Ÿé›†æˆæ¨¡å‹ (æµ‹è¯•é›†):")
        traditional_f1_scores = []
        for dim_name in self.dimensions:
            f1 = self.final_test_results['traditional_ensemble'][dim_name]['f1']
            auc = self.final_test_results['traditional_ensemble'][dim_name]['auc']
            traditional_f1_scores.append(f1)
            print(f"  {self.dim_names[dim_name]}: F1={f1:.3f}, AUC={auc:.3f}")
        
        traditional_avg_f1 = np.mean(traditional_f1_scores)
        print(f"  å¹³å‡F1åˆ†æ•°: {traditional_avg_f1:.3f}")
        
        # æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½
        if self.final_test_results['deep_learning']:
            print(f"\nğŸ§  æ·±åº¦å­¦ä¹ æ¨¡å‹ (æµ‹è¯•é›†):")
            for model_name, results in self.final_test_results['deep_learning'].items():
                print(f"  {model_name}:")
                deep_f1_scores = []
                for dim_name in self.dimensions:
                    f1 = results[dim_name]['f1']
                    auc = results[dim_name]['auc']
                    deep_f1_scores.append(f1)
                    print(f"    {self.dim_names[dim_name]}: F1={f1:.3f}, AUC={auc:.3f}")
                
                deep_avg_f1 = np.mean(deep_f1_scores)
                print(f"    å¹³å‡F1åˆ†æ•°: {deep_avg_f1:.3f}")
        else:
            print(f"\nâš ï¸ æ·±åº¦å­¦ä¹ æ¨¡å‹: ç”±äºç½‘ç»œé—®é¢˜æœªèƒ½è®­ç»ƒ")
        
        return self.final_test_results

def main():
    # æ£€æŸ¥CUDA
    if not torch.cuda.is_available():
        print("âš ï¸ æœªæ£€æµ‹åˆ°CUDAï¼Œå°†æ— æ³•ä½¿ç”¨GPUåŠ é€Ÿ")
        return
    
    # æ•°æ®ç›®å½•
    data_dir = r"C:\Users\lnasl\Desktop\DeepMBTI\data\Text"
    
    # æ£€æŸ¥æ–‡ä»¶
    required_files = [
        'enhanced_english_mbti_train.csv',
        'enhanced_english_mbti_val.csv',
        'enhanced_english_mbti_test.csv'
    ]
    
    for file in required_files:
        if not os.path.exists(os.path.join(data_dir, file)):
            print(f"âŒ ç¼ºå°‘æ–‡ä»¶: {file}")
            return
    
    print("ğŸ® æ£€æµ‹åˆ°RTX 5090ï¼Œå¯ç”¨GPUåŠ é€Ÿè®­ç»ƒï¼")
    print(f"ğŸ’» CPUæ ¸å¿ƒæ•°: {CPU_CORES}")
    
    # åˆ›å»ºé«˜æ€§èƒ½è®­ç»ƒå™¨
    trainer = HighPerformanceMBTITrainer(data_dir)
    
    # è¿è¡Œè®­ç»ƒ
    results = trainer.run_high_performance_training()
    
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  ğŸ¤– ä¼ ç»Ÿæ¨¡å‹: C:\\Users\\lnasl\\Desktop\\DeepMBTI\\TrainedModel\\text\\new\\traditional\\")
    print(f"  ğŸ§  æ·±åº¦æ¨¡å‹: C:\\Users\\lnasl\\Desktop\\DeepMBTI\\TrainedModel\\text\\new\\deep_learning\\")
    print(f"  âš™ï¸ é…ç½®æ–‡ä»¶: C:\\Users\\lnasl\\Desktop\\DeepMBTI\\TrainedModel\\text\\new\\training_config.json")

if __name__ == "__main__":
    main()