import pandas as pd
import numpy as np
import json
import os
import re
import joblib
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class MBTIModelTester:
    def __init__(self, models_dir, data_dir):
        self.models_dir = models_dir
        self.data_dir = data_dir
        self.dimensions = ['E_I', 'S_N', 'T_F', 'J_P']
        self.dim_names = {
            'E_I': 'Extraversion vs Introversion',
            'S_N': 'Sensing vs Intuition', 
            'T_F': 'Thinking vs Feeling',
            'J_P': 'Judging vs Perceiving'
        }
        self.models = {}
        
    def load_models_and_resources(self):
        """åŠ è½½æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹å’Œèµ„æº"""
        print("ğŸ“‚ åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œèµ„æº...")
        
        # 1. åŠ è½½TF-IDFå‘é‡åŒ–å™¨
        tfidf_path = os.path.join(os.path.dirname(self.models_dir), 'tfidf_vectorizer.pkl')
        if os.path.exists(tfidf_path):
            self.tfidf_vectorizer = joblib.load(tfidf_path)
            print(f"âœ… TF-IDFå‘é‡åŒ–å™¨åŠ è½½æˆåŠŸ")
        else:
            print(f"âŒ TF-IDFå‘é‡åŒ–å™¨æœªæ‰¾åˆ°: {tfidf_path}")
            return False
        
        # 2. åŠ è½½å…³é”®è¯æ•°æ®
        keywords_path = os.path.join(self.data_dir, 'english_keywords.json')
        if os.path.exists(keywords_path):
            with open(keywords_path, 'r', encoding='utf-8') as f:
                self.keywords = json.load(f)
            print(f"âœ… å…³é”®è¯æ•°æ®åŠ è½½æˆåŠŸ")
        else:
            print(f"âŒ å…³é”®è¯æ–‡ä»¶æœªæ‰¾åˆ°: {keywords_path}")
            return False
        
        # 3. åŠ è½½å››ä¸ªMBTIç»´åº¦æ¨¡å‹
        for dim in self.dimensions:
            model_path = os.path.join(self.models_dir, f'mbti_{dim}_ensemble.pkl')
            if os.path.exists(model_path):
                self.models[dim] = joblib.load(model_path)
                print(f"âœ… {dim} æ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                print(f"âŒ {dim} æ¨¡å‹æœªæ‰¾åˆ°: {model_path}")
                return False
        
        print(f"ğŸ‰ æ‰€æœ‰æ¨¡å‹å’Œèµ„æºåŠ è½½å®Œæˆï¼")
        return True
    
    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print("\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
        
        test_file = os.path.join(self.data_dir, 'enhanced_english_mbti_test.csv')
        if not os.path.exists(test_file):
            print(f"âŒ æµ‹è¯•æ–‡ä»¶æœªæ‰¾åˆ°: {test_file}")
            return False
        
        self.test_df = pd.read_csv(test_file)
        print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸ: {len(self.test_df):,} æ ·æœ¬")
        
        # åˆ›å»ºMBTIç»´åº¦æ ‡ç­¾
        self.test_df['E_I'] = (self.test_df['type'].str[0] == 'E').astype(int)
        self.test_df['S_N'] = (self.test_df['type'].str[1] == 'S').astype(int)
        self.test_df['T_F'] = (self.test_df['type'].str[2] == 'T').astype(int)
        self.test_df['J_P'] = (self.test_df['type'].str[3] == 'J').astype(int)
        
        # é¢„å¤„ç†æ–‡æœ¬
        self.test_df['processed_text'] = self.test_df['normalized_posts'].fillna('').apply(self.preprocess_text)
        
        return True
    
    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰"""
        if pd.isna(text):
            return ""
        
        text = str(text).lower()
        
        # é«˜æ•ˆçš„æ–‡æœ¬æ¸…ç†
        replacements = {
            r"won't": "will not", r"can't": "cannot", r"n't": " not",
            r"i'm": "i am", r"you're": "you are", r"it's": "it is"
        }
        
        for pattern, replacement in replacements.items():
            text = re.sub(pattern, replacement, text)
        
        # æ¸…ç†URLå’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'http[s]?://\S+', ' ', text)
        text = re.sub(r'[^\w\s\.\!\?\,]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def create_test_features(self):
        """ä¸ºæµ‹è¯•æ•°æ®åˆ›å»ºç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰"""
        print("\nğŸ§  ä¸ºæµ‹è¯•æ•°æ®åˆ›å»ºç‰¹å¾...")
        
        # 1. TF-IDFç‰¹å¾
        print("ğŸ“ åˆ›å»ºTF-IDFç‰¹å¾...")
        test_texts = self.test_df['processed_text'].tolist()
        self.X_test_tfidf = self.tfidf_vectorizer.transform(test_texts)
        print(f"âœ… TF-IDFç‰¹å¾: {self.X_test_tfidf.shape[1]:,} ç»´")
        
        # 2. å…³é”®è¯ç‰¹å¾
        print("ğŸ”‘ åˆ›å»ºå…³é”®è¯ç‰¹å¾...")
        self.X_test_keywords = self.create_keyword_features(self.test_df, 'processed_text')
        print(f"âœ… å…³é”®è¯ç‰¹å¾: {self.X_test_keywords.shape[1]} ç»´")
        
        # 3. ç»Ÿè®¡ç‰¹å¾
        print("ğŸ“Š åˆ›å»ºç»Ÿè®¡ç‰¹å¾...")
        self.X_test_stats = self.create_advanced_stats_features(self.test_df, 'processed_text')
        print(f"âœ… ç»Ÿè®¡ç‰¹å¾: {self.X_test_stats.shape[1]} ç»´")
        
        # 4. ç»„åˆç‰¹å¾
        from scipy import sparse
        self.X_test = sparse.hstack([
            self.X_test_tfidf, 
            self.X_test_keywords, 
            self.X_test_stats
        ])
        
        print(f"ğŸ¯ æ€»ç‰¹å¾ç»´åº¦: {self.X_test.shape[1]:,}")
        return True
    
    def create_keyword_features(self, df, text_column):
        """åˆ›å»ºå…³é”®è¯ç‰¹å¾"""
        features = []
        
        for _, row in df.iterrows():
            text = str(row[text_column]).lower()
            feature_vector = []
            
            for dim in ['E', 'S', 'T', 'J']:
                if dim in self.keywords:
                    keywords_data = self.keywords[dim][:30]  # ä½¿ç”¨æ›´å¤šå…³é”®è¯
                    
                    pos_score = 0
                    neg_score = 0
                    weighted_score = 0
                    
                    for kw_info in keywords_data:
                        keyword = kw_info['keyword']
                        score = kw_info.get('score', 0)
                        importance = kw_info.get('abs_score', abs(score))
                        
                        count = text.count(keyword)
                        if count > 0:
                            if score > 0:
                                pos_score += count * importance
                            else:
                                neg_score += count * importance
                            weighted_score += count * score
                    
                    feature_vector.extend([pos_score, neg_score, weighted_score, pos_score - neg_score])
                else:
                    feature_vector.extend([0, 0, 0, 0])
            
            features.append(feature_vector)
        
        return np.array(features)
    
    def create_advanced_stats_features(self, df, text_column):
        """åˆ›å»ºé«˜çº§ç»Ÿè®¡ç‰¹å¾"""
        features = []
        
        for _, row in df.iterrows():
            text = str(row[text_column])
            words = text.split()
            sentences = text.split('.')
            
            # åŸºç¡€ç»Ÿè®¡
            text_len = len(text)
            word_count = len(words)
            sent_count = len([s for s in sentences if s.strip()])
            
            # é«˜çº§ç‰¹å¾
            avg_word_len = np.mean([len(w) for w in words]) if words else 0
            vocab_diversity = len(set(words)) / len(words) if words else 0
            
            # æ ‡ç‚¹å’Œè¯­è°ƒ
            exclamation_ratio = text.count('!') / text_len if text_len > 0 else 0
            question_ratio = text.count('?') / text_len if text_len > 0 else 0
            caps_ratio = sum(1 for c in text if c.isupper()) / text_len if text_len > 0 else 0
            
            # äººç§°ä»£è¯åˆ†æ
            first_person = (text.count(' i ') + text.count('me') + text.count('my')) / word_count if word_count > 0 else 0
            second_person = (text.count('you') + text.count('your')) / word_count if word_count > 0 else 0
            third_person = (text.count('he') + text.count('she') + text.count('they')) / word_count if word_count > 0 else 0
            
            # æƒ…æ„Ÿè¯æ±‡
            positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'love', 'like', 'enjoy']
            negative_words = ['bad', 'terrible', 'awful', 'hate', 'dislike', 'horrible', 'worst', 'sad', 'angry']
            
            pos_sentiment = sum(text.count(word) for word in positive_words) / word_count if word_count > 0 else 0
            neg_sentiment = sum(text.count(word) for word in negative_words) / word_count if word_count > 0 else 0
            
            # å¤æ‚åº¦æŒ‡æ ‡
            avg_sent_len = word_count / sent_count if sent_count > 0 else 0
            long_words_ratio = len([w for w in words if len(w) > 6]) / word_count if word_count > 0 else 0
            
            stats = [
                text_len, word_count, sent_count, avg_word_len, vocab_diversity,
                exclamation_ratio, question_ratio, caps_ratio,
                first_person, second_person, third_person,
                pos_sentiment, neg_sentiment, avg_sent_len, long_words_ratio
            ]
            
            features.append(stats)
        
        return np.array(features)
    
    def test_all_models(self):
        """æµ‹è¯•æ‰€æœ‰æ¨¡å‹"""
        print("\nğŸ¯ å¼€å§‹æµ‹è¯•æ‰€æœ‰æ¨¡å‹...")
        
        self.test_results = {}
        
        for dim in self.dimensions:
            print(f"\nğŸ“Š æµ‹è¯• {dim} ({self.dim_names[dim]}) æ¨¡å‹...")
            
            # è·å–çœŸå®æ ‡ç­¾
            y_true = self.test_df[dim].values
            
            # è·å–æ¨¡å‹é¢„æµ‹
            model = self.models[dim]
            y_pred = model.predict(self.X_test)
            y_prob = model.predict_proba(self.X_test)[:, 1]
            
            # è®¡ç®—å„ç§æŒ‡æ ‡
            accuracy = accuracy_score(y_true, y_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
            auc = roc_auc_score(y_true, y_prob)
            
            # æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(y_true, y_pred)
            
            self.test_results[dim] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc,
                'confusion_matrix': cm,
                'y_true': y_true,
                'y_pred': y_pred,
                'y_prob': y_prob
            }
            
            print(f"  âœ… å‡†ç¡®ç‡: {accuracy:.3f}")
            print(f"  âœ… ç²¾ç¡®ç‡: {precision:.3f}")
            print(f"  âœ… å¬å›ç‡: {recall:.3f}")
            print(f"  âœ… F1åˆ†æ•°: {f1:.3f}")
            print(f"  âœ… AUC: {auc:.3f}")
        
        return self.test_results
    
    def display_detailed_results(self):
        """æ˜¾ç¤ºè¯¦ç»†çš„æµ‹è¯•ç»“æœ"""
        print("\n" + "="*80)
        print("ğŸ“Š MBTIæ¨¡å‹æµ‹è¯•ç»“æœè¯¦ç»†æŠ¥å‘Š")
        print("="*80)
        
        # æ€»ä½“æ€§èƒ½æ¦‚è§ˆ
        print(f"\nğŸ¯ æ€»ä½“æ€§èƒ½æ¦‚è§ˆ:")
        print(f"{'ç»´åº¦':<15} {'å‡†ç¡®ç‡':<8} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'AUC':<8}")
        print("-" * 65)
        
        all_accuracies = []
        all_f1_scores = []
        all_aucs = []
        
        for dim in self.dimensions:
            results = self.test_results[dim]
            all_accuracies.append(results['accuracy'])
            all_f1_scores.append(results['f1'])
            all_aucs.append(results['auc'])
            
            print(f"{dim:<15} {results['accuracy']:<8.3f} {results['precision']:<8.3f} "
                  f"{results['recall']:<8.3f} {results['f1']:<8.3f} {results['auc']:<8.3f}")
        
        print("-" * 65)
        print(f"{'å¹³å‡':<15} {np.mean(all_accuracies):<8.3f} {'':<8} {'':<8} "
              f"{np.mean(all_f1_scores):<8.3f} {np.mean(all_aucs):<8.3f}")
        
        # å„ç»´åº¦è¯¦ç»†åˆ†æ
        for dim in self.dimensions:
            print(f"\nğŸ” {dim} ({self.dim_names[dim]}) è¯¦ç»†åˆ†æ:")
            results = self.test_results[dim]
            
            # åˆ†ç±»æŠ¥å‘Š
            print(f"\nåˆ†ç±»æŠ¥å‘Š:")
            target_names = ['0', '1']
            if dim == 'E_I':
                target_names = ['Introversion', 'Extraversion']
            elif dim == 'S_N':
                target_names = ['Sensing', 'Intuition']
            elif dim == 'T_F':
                target_names = ['Thinking', 'Feeling']
            elif dim == 'J_P':
                target_names = ['Judging', 'Perceiving']
            
            print(classification_report(results['y_true'], results['y_pred'], 
                                      target_names=target_names, digits=3))
            
            # æ··æ·†çŸ©é˜µ
            print(f"æ··æ·†çŸ©é˜µ:")
            cm = results['confusion_matrix']
            print(f"å®é™…\\é¢„æµ‹  {target_names[0]:<12} {target_names[1]:<12}")
            print(f"{target_names[0]:<12} {cm[0][0]:<12} {cm[0][1]:<12}")
            print(f"{target_names[1]:<12} {cm[1][0]:<12} {cm[1][1]:<12}")
            
            # æ ·æœ¬åˆ†å¸ƒ
            total_samples = len(results['y_true'])
            positive_samples = sum(results['y_true'])
            negative_samples = total_samples - positive_samples
            
            print(f"\næ ·æœ¬åˆ†å¸ƒ:")
            print(f"  {target_names[0]}: {negative_samples} æ ·æœ¬ ({negative_samples/total_samples*100:.1f}%)")
            print(f"  {target_names[1]}: {positive_samples} æ ·æœ¬ ({positive_samples/total_samples*100:.1f}%)")
    
    def predict_mbti_type(self, sample_texts):
        """é¢„æµ‹æ–°æ–‡æœ¬çš„MBTIç±»å‹"""
        print(f"\nğŸ”® é¢„æµ‹æ–°æ–‡æœ¬çš„MBTIç±»å‹...")
        
        if isinstance(sample_texts, str):
            sample_texts = [sample_texts]
        
        # é¢„å¤„ç†æ–‡æœ¬
        processed_texts = [self.preprocess_text(text) for text in sample_texts]
        
        # åˆ›å»ºä¸´æ—¶DataFrame
        temp_df = pd.DataFrame({'processed_text': processed_texts})
        
        # åˆ›å»ºç‰¹å¾
        # TF-IDF
        X_tfidf = self.tfidf_vectorizer.transform(processed_texts)
        
        # å…³é”®è¯ç‰¹å¾
        X_keywords = self.create_keyword_features(temp_df, 'processed_text')
        
        # ç»Ÿè®¡ç‰¹å¾
        X_stats = self.create_advanced_stats_features(temp_df, 'processed_text')
        
        # ç»„åˆç‰¹å¾
        from scipy import sparse
        X_combined = sparse.hstack([X_tfidf, X_keywords, X_stats])
        
        # é¢„æµ‹æ¯ä¸ªç»´åº¦
        predictions = {}
        probabilities = {}
        
        for dim in self.dimensions:
            model = self.models[dim]
            pred = model.predict(X_combined)
            prob = model.predict_proba(X_combined)[:, 1]
            
            predictions[dim] = pred
            probabilities[dim] = prob
        
        # ç»„åˆæˆMBTIç±»å‹
        mbti_types = []
        for i in range(len(sample_texts)):
            mbti_type = ""
            mbti_type += "E" if predictions['E_I'][i] == 1 else "I"
            mbti_type += "S" if predictions['S_N'][i] == 1 else "N"
            mbti_type += "T" if predictions['T_F'][i] == 1 else "F"
            mbti_type += "J" if predictions['J_P'][i] == 1 else "P"
            mbti_types.append(mbti_type)
        
        # æ˜¾ç¤ºç»“æœ
        for i, (text, mbti_type) in enumerate(zip(sample_texts, mbti_types)):
            print(f"\næ ·æœ¬ {i+1}:")
            print(f"æ–‡æœ¬: {text[:100]}{'...' if len(text) > 100 else ''}")
            print(f"é¢„æµ‹MBTIç±»å‹: {mbti_type}")
            print(f"å„ç»´åº¦æ¦‚ç‡:")
            for dim in self.dimensions:
                prob = probabilities[dim][i]
                print(f"  {dim}: {prob:.3f}")
        
        return mbti_types, probabilities
    
    def run_complete_test(self):
        """è¿è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹"""
        start_time = datetime.now()
        
        print("ğŸš€ å¼€å§‹MBTIæ¨¡å‹å®Œæ•´æµ‹è¯•...")
        
        # 1. åŠ è½½æ¨¡å‹å’Œèµ„æº
        if not self.load_models_and_resources():
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return False
        
        # 2. åŠ è½½æµ‹è¯•æ•°æ®
        if not self.load_test_data():
            print("âŒ æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return False
        
        # 3. åˆ›å»ºæµ‹è¯•ç‰¹å¾
        if not self.create_test_features():
            print("âŒ ç‰¹å¾åˆ›å»ºå¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return False
        
        # 4. æµ‹è¯•æ‰€æœ‰æ¨¡å‹
        self.test_all_models()
        
        # 5. æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        self.display_detailed_results()
        
        end_time = datetime.now()
        test_time = end_time - start_time
        
        print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print(f"â±ï¸ æ€»æµ‹è¯•æ—¶é—´: {test_time}")
        
        return True

def main():
    # è®¾ç½®è·¯å¾„
    models_dir = r"C:\Users\lnasl\Desktop\DeepMBTI\TrainedModel\text\new\traditional"
    data_dir = r"C:\Users\lnasl\Desktop\DeepMBTI\data\Text"
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(models_dir):
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
        return
    
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = MBTIModelTester(models_dir, data_dir)
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    success = tester.run_complete_test()
    
    if success:
        print(f"\nğŸ’¡ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿›è¡Œæ–°æ–‡æœ¬é¢„æµ‹:")
        print(f"sample_text = 'Your text here...'")
        print(f"mbti_types, probs = tester.predict_mbti_type(sample_text)")

if __name__ == "__main__":
    main()