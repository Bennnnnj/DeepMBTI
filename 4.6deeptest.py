import pandas as pd
import numpy as np
import json
import os
import re
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥transformers
try:
    from transformers import AutoTokenizer, AutoModel
    from transformers import DistilBertTokenizer, DistilBertModel, RobertaTokenizer, RobertaModel
    HAS_TRANSFORMERS = True
except ImportError:
    print("âš ï¸ transformersåº“æœªå®‰è£…ï¼Œè¯·å®‰è£…: pip install transformers")
    HAS_TRANSFORMERS = False

# PyTorchæ•°æ®é›†ç±»
class MBTIDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        labels = torch.tensor(self.labels[idx], dtype=torch.float32) if self.labels is not None else None
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        result = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten()
        }
        
        if labels is not None:
            result['labels'] = labels
            
        return result

# æ·±åº¦å­¦ä¹ æ¨¡å‹å®šä¹‰
class MBTITransformerModel(nn.Module):
    def __init__(self, model_name='distilbert-base-uncased', num_labels=4):
        super().__init__()
        
        if 'distilbert' in model_name:
            self.bert = DistilBertModel.from_pretrained(model_name)
            hidden_size = self.bert.config.hidden_size
        elif 'roberta' in model_name:
            self.bert = RobertaModel.from_pretrained(model_name)
            hidden_size = self.bert.config.hidden_size
        else:
            self.bert = AutoModel.from_pretrained(model_name)
            hidden_size = self.bert.config.hidden_size
        
        # å¤šå±‚åˆ†ç±»å¤´
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, num_labels)
        )
    
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state[:, 0]  # CLS token
        output = self.dropout(pooled_output)
        return self.classifier(output)

class MBTIDeepModelTester:
    def __init__(self, models_dir, data_dir):
        self.models_dir = models_dir
        self.data_dir = data_dir
        self.dimensions = ['E_I', 'S_N', 'T_F', 'J_P']
        self.dim_names = {
            'E_I': 'Extraversion vs Introversion',
            'S_N': 'Sensing vs Intuition', 
            'T_F': 'Thinking vs Feeling',
            'J_P': 'Judging vs Perceiving'
        }
        self.models = {}
        self.tokenizers = {}
        self.device = self.setup_device()
        
    def setup_device(self):
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if torch.cuda.is_available():
            device = torch.device('cuda')
            print(f"ğŸ® ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
        else:
            device = torch.device('cpu')
            print("ğŸ’» ä½¿ç”¨CPU")
        return device
    
    def preprocess_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰"""
        if pd.isna(text):
            return ""
        
        text = str(text).lower()
        
        # é«˜æ•ˆçš„æ–‡æœ¬æ¸…ç†
        replacements = {
            r"won't": "will not", r"can't": "cannot", r"n't": " not",
            r"i'm": "i am", r"you're": "you are", r"it's": "it is"
        }
        
        for pattern, replacement in replacements.items():
            text = re.sub(pattern, replacement, text)
        
        # æ¸…ç†URLå’Œç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'http[s]?://\S+', ' ', text)
        text = re.sub(r'[^\w\s\.\!\?\,]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def discover_models(self):
        """è‡ªåŠ¨å‘ç°å¯ç”¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹"""
        print("ğŸ” æœç´¢å¯ç”¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹...")
        
        available_models = []
        
        # æ£€æŸ¥deep_learningç›®å½•ä¸­çš„æ‰€æœ‰å­ç›®å½•
        if os.path.exists(self.models_dir):
            for item in os.listdir(self.models_dir):
                model_path = os.path.join(self.models_dir, item)
                if os.path.isdir(model_path):
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶
                    model_file = os.path.join(model_path, 'model.pt')
                    config_file = os.path.join(model_path, 'config.json')
                    tokenizer_file = os.path.join(model_path, 'tokenizer.json')
                    
                    if os.path.exists(model_file):
                        available_models.append({
                            'name': item,
                            'path': model_path,
                            'model_file': model_file,
                            'has_config': os.path.exists(config_file),
                            'has_tokenizer': os.path.exists(tokenizer_file)
                        })
                        print(f"  âœ… å‘ç°æ¨¡å‹: {item}")
        
        if not available_models:
            print("âŒ æœªå‘ç°ä»»ä½•æ·±åº¦å­¦ä¹ æ¨¡å‹")
            return []
        
        print(f"ğŸ‰ å…±å‘ç° {len(available_models)} ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹")
        return available_models
    
    def load_deep_models(self):
        """åŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹"""
        print("\nğŸ“‚ åŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹...")
        
        if not HAS_TRANSFORMERS:
            print("âŒ transformersåº“æœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ·±åº¦å­¦ä¹ æ¨¡å‹")
            return False
        
        # å‘ç°å¯ç”¨æ¨¡å‹
        available_models = self.discover_models()
        if not available_models:
            return False
        
        self.loaded_models = {}
        
        for model_info in available_models:
            model_name = model_info['name']
            model_path = model_info['path']
            model_file = model_info['model_file']
            
            print(f"\nğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
            
            try:
                # å°è¯•ä»æ¨¡å‹åç§°æ¨æ–­tokenizerç±»å‹
                if 'distilbert' in model_name.lower():
                    base_model = 'distilbert-base-uncased'
                    tokenizer = DistilBertTokenizer.from_pretrained(model_path)
                elif 'roberta' in model_name.lower():
                    base_model = 'roberta-base'
                    tokenizer = RobertaTokenizer.from_pretrained(model_path)
                else:
                    # é»˜è®¤ä½¿ç”¨DistilBERT
                    base_model = 'distilbert-base-uncased'
                    try:
                        tokenizer = AutoTokenizer.from_pretrained(model_path)
                    except:
                        print(f"  âš ï¸ æ— æ³•ä»{model_path}åŠ è½½tokenizerï¼Œå°è¯•ä½¿ç”¨é¢„è®­ç»ƒtokenizer")
                        tokenizer = DistilBertTokenizer.from_pretrained(base_model)
                
                print(f"  âœ… TokenizeråŠ è½½æˆåŠŸ")
                
                # åˆ›å»ºæ¨¡å‹å®ä¾‹
                model = MBTITransformerModel(base_model, num_labels=4)
                
                # åŠ è½½æ¨¡å‹æƒé‡
                state_dict = torch.load(model_file, map_location=self.device)
                
                # å¦‚æœæ¨¡å‹æ˜¯DataParallelåŒ…è£…çš„ï¼Œéœ€è¦å»æ‰'module.'å‰ç¼€
                if any(key.startswith('module.') for key in state_dict.keys()):
                    new_state_dict = {}
                    for key, value in state_dict.items():
                        new_key = key.replace('module.', '')
                        new_state_dict[new_key] = value
                    state_dict = new_state_dict
                
                model.load_state_dict(state_dict)
                model = model.to(self.device)
                model.eval()
                
                self.loaded_models[model_name] = {
                    'model': model,
                    'tokenizer': tokenizer,
                    'base_model': base_model
                }
                
                print(f"  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
                
            except Exception as e:
                print(f"  âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                continue
        
        if not self.loaded_models:
            print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ¨¡å‹")
            return False
        
        print(f"\nğŸ‰ æˆåŠŸåŠ è½½ {len(self.loaded_models)} ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹")
        return True
    
    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print("\nğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
        
        test_file = os.path.join(self.data_dir, 'enhanced_english_mbti_test.csv')
        if not os.path.exists(test_file):
            print(f"âŒ æµ‹è¯•æ–‡ä»¶æœªæ‰¾åˆ°: {test_file}")
            return False
        
        self.test_df = pd.read_csv(test_file)
        print(f"âœ… æµ‹è¯•æ•°æ®åŠ è½½æˆåŠŸ: {len(self.test_df):,} æ ·æœ¬")
        
        # åˆ›å»ºMBTIç»´åº¦æ ‡ç­¾
        self.test_df['E_I'] = (self.test_df['type'].str[0] == 'E').astype(int)
        self.test_df['S_N'] = (self.test_df['type'].str[1] == 'S').astype(int)
        self.test_df['T_F'] = (self.test_df['type'].str[2] == 'T').astype(int)
        self.test_df['J_P'] = (self.test_df['type'].str[3] == 'J').astype(int)
        
        # é¢„å¤„ç†æ–‡æœ¬
        self.test_df['processed_text'] = self.test_df['normalized_posts'].fillna('').apply(self.preprocess_text)
        
        return True
    
    def test_model(self, model_name, model_data):
        """æµ‹è¯•å•ä¸ªæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        print(f"\nğŸ¯ æµ‹è¯•æ¨¡å‹: {model_name}")
        
        model = model_data['model']
        tokenizer = model_data['tokenizer']
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_labels = self.test_df[self.dimensions].values
        test_dataset = MBTIDataset(
            self.test_df['processed_text'].tolist(),
            test_labels,
            tokenizer,
            max_length=256
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=32,  # ä½¿ç”¨è¾ƒå°çš„æ‰¹é‡é¿å…å†…å­˜é—®é¢˜
            shuffle=False,
            num_workers=2 if os.name != 'nt' else 0,  # Windowså…¼å®¹æ€§
            pin_memory=True if torch.cuda.is_available() else False
        )
        
        # é¢„æµ‹
        model.eval()
        all_predictions = []
        all_probabilities = []
        all_labels = []
        
        print("  ğŸ”„ è¿›è¡Œé¢„æµ‹...")
        with torch.no_grad():
            for batch_idx, batch in enumerate(test_loader):
                if batch_idx % 50 == 0:
                    print(f"    å¤„ç†æ‰¹æ¬¡ {batch_idx}/{len(test_loader)}")
                
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                try:
                    if torch.cuda.is_available():
                        with torch.cuda.amp.autocast():
                            outputs = model(input_ids, attention_mask)
                    else:
                        outputs = model(input_ids, attention_mask)
                    
                    probabilities = torch.sigmoid(outputs)
                    predictions = (probabilities > 0.5).float()
                    
                    all_predictions.append(predictions.cpu())
                    all_probabilities.append(probabilities.cpu())
                    all_labels.append(labels.cpu())
                    
                except Exception as e:
                    print(f"    âš ï¸ æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                    continue
        
        if not all_predictions:
            print(f"  âŒ æ¨¡å‹ {model_name} é¢„æµ‹å¤±è´¥")
            return None
        
        # åˆå¹¶ç»“æœ
        predictions = torch.cat(all_predictions, dim=0).numpy()
        probabilities = torch.cat(all_probabilities, dim=0).numpy()
        true_labels = torch.cat(all_labels, dim=0).numpy()
        
        print("  âœ… é¢„æµ‹å®Œæˆï¼Œè®¡ç®—æŒ‡æ ‡...")
        
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æ€§èƒ½
        model_results = {}
        for i, dim_name in enumerate(self.dimensions):
            y_true = true_labels[:, i]
            y_pred = predictions[:, i]
            y_prob = probabilities[:, i]
            
            accuracy = accuracy_score(y_true, y_pred)
            precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
            auc = roc_auc_score(y_true, y_prob)
            cm = confusion_matrix(y_true, y_pred)
            
            model_results[dim_name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'auc': auc,
                'confusion_matrix': cm,
                'y_true': y_true,
                'y_pred': y_pred,
                'y_prob': y_prob
            }
            
            print(f"    {dim_name}: F1={f1:.3f}, AUC={auc:.3f}")
        
        return model_results
    
    def test_all_models(self):
        """æµ‹è¯•æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹"""
        print("\nğŸš€ å¼€å§‹æµ‹è¯•æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¨¡å‹...")
        
        self.all_results = {}
        
        for model_name, model_data in self.loaded_models.items():
            try:
                results = self.test_model(model_name, model_data)
                if results:
                    self.all_results[model_name] = results
                    print(f"âœ… {model_name} æµ‹è¯•å®Œæˆ")
                else:
                    print(f"âŒ {model_name} æµ‹è¯•å¤±è´¥")
            except Exception as e:
                print(f"âŒ {model_name} æµ‹è¯•å¼‚å¸¸: {e}")
                continue
            finally:
                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        
        return self.all_results
    
    def display_results(self):
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœ"""
        print("\n" + "="*100)
        print("ğŸ§  æ·±åº¦å­¦ä¹ MBTIæ¨¡å‹æµ‹è¯•ç»“æœ")
        print("="*100)
        
        if not self.all_results:
            print("âŒ æ²¡æœ‰å¯æ˜¾ç¤ºçš„ç»“æœ")
            return
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹æ˜¾ç¤ºç»“æœ
        for model_name, model_results in self.all_results.items():
            print(f"\nğŸ¤– æ¨¡å‹: {model_name}")
            print("="*60)
            
            # æ€§èƒ½æ¦‚è§ˆè¡¨
            print(f"{'ç»´åº¦':<15} {'å‡†ç¡®ç‡':<8} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'AUC':<8}")
            print("-" * 65)
            
            all_f1 = []
            all_auc = []
            
            for dim in self.dimensions:
                results = model_results[dim]
                all_f1.append(results['f1'])
                all_auc.append(results['auc'])
                
                print(f"{dim:<15} {results['accuracy']:<8.3f} {results['precision']:<8.3f} "
                      f"{results['recall']:<8.3f} {results['f1']:<8.3f} {results['auc']:<8.3f}")
            
            print("-" * 65)
            print(f"{'å¹³å‡':<15} {'':<8} {'':<8} {'':<8} {np.mean(all_f1):<8.3f} {np.mean(all_auc):<8.3f}")
            
            # è¯¦ç»†åˆ†æ
            for dim in self.dimensions:
                print(f"\nğŸ“Š {dim} ({self.dim_names[dim]}) è¯¦ç»†åˆ†æ:")
                results = model_results[dim]
                
                # ç›®æ ‡åç§°
                target_names = ['0', '1']
                if dim == 'E_I':
                    target_names = ['Introversion', 'Extraversion']
                elif dim == 'S_N':
                    target_names = ['Sensing', 'Intuition']
                elif dim == 'T_F':
                    target_names = ['Thinking', 'Feeling']
                elif dim == 'J_P':
                    target_names = ['Judging', 'Perceiving']
                
                # åˆ†ç±»æŠ¥å‘Š
                print(classification_report(results['y_true'], results['y_pred'], 
                                          target_names=target_names, digits=3))
                
                # æ··æ·†çŸ©é˜µ
                print("æ··æ·†çŸ©é˜µ:")
                cm = results['confusion_matrix']
                print(f"å®é™…\\é¢„æµ‹  {target_names[0]:<12} {target_names[1]:<12}")
                print(f"{target_names[0]:<12} {cm[0][0]:<12} {cm[0][1]:<12}")
                print(f"{target_names[1]:<12} {cm[1][0]:<12} {cm[1][1]:<12}")
        
        # æ¨¡å‹æ¯”è¾ƒ
        if len(self.all_results) > 1:
            print(f"\nğŸ“ˆ æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ:")
            print("="*60)
            print(f"{'æ¨¡å‹':<20} {'å¹³å‡F1':<10} {'å¹³å‡AUC':<10}")
            print("-" * 40)
            
            for model_name, model_results in self.all_results.items():
                avg_f1 = np.mean([model_results[dim]['f1'] for dim in self.dimensions])
                avg_auc = np.mean([model_results[dim]['auc'] for dim in self.dimensions])
                print(f"{model_name:<20} {avg_f1:<10.3f} {avg_auc:<10.3f}")
    
    def predict_sample_text(self, text, model_name=None):
        """ç”¨æŒ‡å®šæ¨¡å‹é¢„æµ‹æ ·æœ¬æ–‡æœ¬"""
        if not self.loaded_models:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")
            return None
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª
        if model_name is None:
            model_name = list(self.loaded_models.keys())[0]
        
        if model_name not in self.loaded_models:
            print(f"âŒ æ¨¡å‹ {model_name} ä¸å­˜åœ¨")
            print(f"å¯ç”¨æ¨¡å‹: {list(self.loaded_models.keys())}")
            return None
        
        print(f"\nğŸ”® ä½¿ç”¨æ¨¡å‹ {model_name} é¢„æµ‹æ–‡æœ¬...")
        
        model_data = self.loaded_models[model_name]
        model = model_data['model']
        tokenizer = model_data['tokenizer']
        
        # é¢„å¤„ç†æ–‡æœ¬
        processed_text = self.preprocess_text(text)
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = MBTIDataset([processed_text], None, tokenizer, max_length=256)
        dataloader = DataLoader(dataset, batch_size=1, shuffle=False)
        
        # é¢„æµ‹
        model.eval()
        with torch.no_grad():
            batch = next(iter(dataloader))
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            
            outputs = model(input_ids, attention_mask)
            probabilities = torch.sigmoid(outputs).cpu().numpy()[0]
            predictions = (probabilities > 0.5).astype(int)
        
        # æ„å»ºMBTIç±»å‹
        mbti_type = ""
        mbti_type += "E" if predictions[0] == 1 else "I"
        mbti_type += "S" if predictions[1] == 1 else "N"
        mbti_type += "T" if predictions[2] == 1 else "F"
        mbti_type += "J" if predictions[3] == 1 else "P"
        
        print(f"åŸå§‹æ–‡æœ¬: {text}")
        print(f"é¢„æµ‹MBTIç±»å‹: {mbti_type}")
        print(f"å„ç»´åº¦æ¦‚ç‡:")
        for i, dim in enumerate(self.dimensions):
            print(f"  {dim}: {probabilities[i]:.3f}")
        
        return mbti_type, probabilities
    
    def run_complete_test(self):
        """è¿è¡Œå®Œæ•´çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æµ‹è¯•"""
        start_time = datetime.now()
        
        print("ğŸš€ å¼€å§‹æ·±åº¦å­¦ä¹ MBTIæ¨¡å‹æµ‹è¯•...")
        
        # 1. åŠ è½½æ¨¡å‹
        if not self.load_deep_models():
            print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return False
        
        # 2. åŠ è½½æµ‹è¯•æ•°æ®
        if not self.load_test_data():
            print("âŒ æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
            return False
        
        # 3. æµ‹è¯•æ‰€æœ‰æ¨¡å‹
        self.test_all_models()
        
        # 4. æ˜¾ç¤ºç»“æœ
        self.display_results()
        
        end_time = datetime.now()
        test_time = end_time - start_time
        
        print(f"\nğŸ‰ æ·±åº¦å­¦ä¹ æ¨¡å‹æµ‹è¯•å®Œæˆï¼")
        print(f"â±ï¸ æ€»æµ‹è¯•æ—¶é—´: {test_time}")
        
        return True

def main():
    # è®¾ç½®è·¯å¾„
    models_dir = r"C:\Users\lnasl\Desktop\DeepMBTI\TrainedModel\text\new\deep_learning"
    data_dir = r"C:\Users\lnasl\Desktop\DeepMBTI\data\Text"
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(models_dir):
        print(f"âŒ æ·±åº¦å­¦ä¹ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
        return
    
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = MBTIDeepModelTester(models_dir, data_dir)
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    success = tester.run_complete_test()
    
    if success:
        print(f"\nğŸ’¡ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿›è¡Œæ–°æ–‡æœ¬é¢„æµ‹:")
        print(f"sample_text = 'Your text here...'")
        print(f"mbti_type, probs = tester.predict_sample_text(sample_text)")
        
        # ç¤ºä¾‹é¢„æµ‹
        if hasattr(tester, 'loaded_models') and tester.loaded_models:
            print(f"\nğŸ”® ç¤ºä¾‹é¢„æµ‹:")
            sample_text = "I love meeting new people and going to parties. I always speak up in meetings and enjoy being the center of attention."
            try:
                tester.predict_sample_text(sample_text)
            except Exception as e:
                print(f"ç¤ºä¾‹é¢„æµ‹å¤±è´¥: {e}")

if __name__ == "__main__":
    main()