import pandas as pd
import numpy as np
import re
import nltk
from collections import defaultdict, Counter
import os
import json
import warnings
from datetime import datetime
import random
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import textstat
warnings.filterwarnings('ignore')

class EnglishMBTIDatasetProcessor:
    def __init__(self):
        # è‹±æ–‡åœç”¨è¯
        try:
            from nltk.corpus import stopwords
            self.english_stopwords = set(stopwords.words('english'))
        except:
            self.english_stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}
        
        # MBTIè‹±æ–‡å…³é”®è¯åº“
        self.mbti_keywords = {
            'E': {
                'social', 'party', 'people', 'outgoing', 'energetic', 'talkative', 
                'expressive', 'collaborative', 'gregarious', 'vivacious', 'animated', 
                'enthusiastic', 'spontaneous', 'assertive', 'confident', 'leadership', 
                'presentation', 'performance', 'audience', 'community', 'society', 
                'external', 'together', 'group', 'team', 'networking', 'meeting', 
                'conversation', 'chat', 'interaction', 'communicate', 'express'
            },
            'I': {
                'alone', 'quiet', 'solitude', 'private', 'reserved', 'withdrawn', 
                'shy', 'independent', 'individual', 'personal', 'calm', 'peaceful', 
                'reflection', 'thinking', 'contemplation', 'reading', 'writing', 
                'meditation', 'introspection', 'solitary', 'reclusive', 'secluded', 
                'intimate', 'inner', 'internal', 'thoughtful', 'pensive', 'reflective', 
                'deliberate', 'careful', 'selective', 'depth', 'concentrate', 'focus', 
                'privacy', 'sanctuary', 'retreat', 'recharge', 'observe', 'listen'
            },
            'S': {
                'practical', 'realistic', 'concrete', 'facts', 'details', 'specific', 
                'experience', 'present', 'actual', 'real', 'tangible', 'sequential', 
                'traditional', 'conventional', 'established', 'proven', 'tested', 
                'observable', 'measurable', 'precise', 'literal', 'factual', 'empirical', 
                'hands-on', 'technical', 'methodical', 'systematic', 'routine', 
                'procedure', 'protocol', 'evidence', 'data', 'statistics', 'measurement', 
                'calculation', 'implementation', 'application', 'utility', 'function'
            },
            'N': {
                'future', 'possibility', 'potential', 'imagination', 'creative', 
                'innovative', 'abstract', 'concept', 'theory', 'idea', 'vision', 
                'intuition', 'insight', 'pattern', 'meaning', 'symbolic', 'metaphor', 
                'inspiration', 'dream', 'fantasy', 'original', 'conceptual', 
                'theoretical', 'hypothetical', 'speculative', 'inventive', 'artistic', 
                'imaginative', 'visionary', 'trend', 'forecast', 'prediction', 
                'anticipate', 'foresee', 'breakthrough', 'revolution', 'transformation'
            },
            'T': {
                'logical', 'rational', 'analysis', 'objective', 'reason', 'logic', 
                'critical', 'systematic', 'analytical', 'problem-solving', 'efficiency', 
                'effectiveness', 'truth', 'fact', 'evidence', 'principle', 'criteria', 
                'standard', 'judge', 'evaluate', 'methodical', 'strategic', 'tactical', 
                'algorithmic', 'mathematical', 'scientific', 'research', 'investigation', 
                'diagnosis', 'solution', 'optimization', 'performance', 'metrics', 
                'benchmark', 'assessment', 'examination', 'reasoning', 'calculation'
            },
            'F': {
                'feeling', 'emotion', 'heart', 'empathy', 'compassion', 'care', 
                'love', 'harmony', 'relationship', 'personal', 'values', 'moral', 
                'ethical', 'human', 'understanding', 'sympathy', 'kindness', 'warmth', 
                'support', 'help', 'concern', 'emotional', 'sensitive', 'nurturing', 
                'supportive', 'considerate', 'diplomatic', 'cooperative', 'collaborative', 
                'consensus', 'welfare', 'wellbeing', 'happiness', 'satisfaction', 
                'fulfillment', 'motivation', 'inspiration', 'encouragement', 'appreciation'
            },
            'J': {
                'organized', 'planned', 'schedule', 'structure', 'order', 'control', 
                'decided', 'settled', 'closure', 'deadline', 'goal', 'target', 
                'systematic', 'methodical', 'routine', 'regular', 'predictable', 
                'determined', 'decisive', 'complete', 'finish', 'accomplish', 
                'disciplined', 'punctual', 'orderly', 'neat', 'tidy', 'arranged', 
                'timeline', 'milestone', 'achievement', 'completion', 'resolution', 
                'commitment', 'responsibility', 'accountability', 'reliability'
            },
            'P': {
                'flexible', 'adaptable', 'spontaneous', 'open-ended', 'casual', 
                'relaxed', 'informal', 'freedom', 'options', 'alternatives', 'explore', 
                'discover', 'experiment', 'playful', 'curious', 'improvise', 'adjust', 
                'change', 'variety', 'diverse', 'versatile', 'dynamic', 'fluid', 
                'elastic', 'malleable', 'opportunistic', 'serendipity', 'adventure', 
                'exploration', 'experimentation', 'innovation', 'creativity', 'openness'
            }
        }
        
        # æ•°æ®å¢å¼ºé…ç½®
        self.augmentation_methods = {
            'synonym_replacement': True,
            'random_insertion': True,
            'random_swap': True,
            'random_deletion': True,
            'sentence_reordering': True,
            'paraphrasing': True
        }
    
    def comprehensive_data_analysis(self, df):
        """å…¨é¢çš„æ•°æ®åˆ†æ"""
        print("=== æ‰§è¡Œå…¨é¢æ•°æ®åˆ†æ ===")
        
        analysis_report = {
            'basic_statistics': {},
            'distribution_analysis': {},
            'quality_assessment': {},
            'text_characteristics': {},
            'mbti_patterns': {},
            'recommendations': []
        }
        
        # 1. åŸºç¡€ç»Ÿè®¡
        analysis_report['basic_statistics'] = {
            'total_samples': len(df),
            'columns': list(df.columns),
            'missing_values': df.isnull().sum().to_dict(),
            'data_types': df.dtypes.to_dict()
        }
        
        # 2. MBTIåˆ†å¸ƒåˆ†æ
        if 'type' in df.columns:
            type_counts = df['type'].value_counts()
            analysis_report['distribution_analysis'] = {
                'mbti_distribution': type_counts.to_dict(),
                'most_common_type': type_counts.index[0],
                'least_common_type': type_counts.index[-1],
                'imbalance_ratio': type_counts.max() / type_counts.min(),
                'dimension_distributions': self.analyze_dimension_distributions(df)
            }
        
        # 3. æ–‡æœ¬ç‰¹å¾åˆ†æ
        if 'posts' in df.columns:
            text_analysis = self.analyze_text_characteristics(df['posts'])
            analysis_report['text_characteristics'] = text_analysis
        
        # 4. æ•°æ®è´¨é‡è¯„ä¼°
        quality_assessment = self.assess_data_quality(df)
        analysis_report['quality_assessment'] = quality_assessment
        
        return analysis_report
    
    def analyze_dimension_distributions(self, df):
        """åˆ†æMBTIç»´åº¦åˆ†å¸ƒ"""
        dimensions = {
            'E_vs_I': {'E': 0, 'I': 0},
            'S_vs_N': {'S': 0, 'N': 0},
            'T_vs_F': {'T': 0, 'F': 0},
            'J_vs_P': {'J': 0, 'P': 0}
        }
        
        for mbti_type in df['type']:
            if len(mbti_type) == 4:
                dimensions['E_vs_I'][mbti_type[0]] += 1
                dimensions['S_vs_N'][mbti_type[1]] += 1
                dimensions['T_vs_F'][mbti_type[2]] += 1
                dimensions['J_vs_P'][mbti_type[3]] += 1
        
        # è®¡ç®—å¹³è¡¡åº¦
        for dim_name, counts in dimensions.items():
            total = sum(counts.values())
            if total > 0:
                balance_score = min(counts.values()) / max(counts.values()) if max(counts.values()) > 0 else 0
                dimensions[dim_name]['balance_score'] = balance_score
                dimensions[dim_name]['imbalance_ratio'] = max(counts.values()) / min(counts.values()) if min(counts.values()) > 0 else float('inf')
        
        return dimensions
    
    def analyze_text_characteristics(self, texts):
        """åˆ†ææ–‡æœ¬ç‰¹å¾"""
        text_features = {
            'total_texts': len(texts),
            'empty_texts': texts.isnull().sum(),
            'avg_length': texts.str.len().mean(),
            'median_length': texts.str.len().median(),
            'std_length': texts.str.len().std(),
            'min_length': texts.str.len().min(),
            'max_length': texts.str.len().max(),
            'length_distribution': {
                'very_short': (texts.str.len() < 100).sum(),
                'short': ((texts.str.len() >= 100) & (texts.str.len() < 500)).sum(),
                'medium': ((texts.str.len() >= 500) & (texts.str.len() < 2000)).sum(),
                'long': ((texts.str.len() >= 2000) & (texts.str.len() < 5000)).sum(),
                'very_long': (texts.str.len() >= 5000).sum()
            }
        }
        
        # è®¡ç®—å¹³å‡å¯è¯»æ€§ï¼ˆé‡‡æ ·ï¼‰
        readability_scores = []
        for text in texts.dropna().head(1000):
            if text and len(text) > 10:
                try:
                    readability = textstat.flesch_reading_ease(text)
                    readability_scores.append(readability)
                except:
                    continue
        
        if readability_scores:
            text_features['avg_readability'] = np.mean(readability_scores)
        
        return text_features
    
    def assess_data_quality(self, df):
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        quality_report = {}
        
        # 1. å®Œæ•´æ€§
        total_cells = df.shape[0] * df.shape[1]
        missing_cells = df.isnull().sum().sum()
        completeness = 1 - (missing_cells / total_cells)
        quality_report['completeness'] = completeness
        
        # 2. ä¸€è‡´æ€§
        consistency_issues = 0
        if 'type' in df.columns:
            valid_types = df['type'].str.match(r'^[EINS][INST][TF][JP]$')
            consistency_issues += (~valid_types).sum()
        
        consistency = 1 - (consistency_issues / len(df))
        quality_report['consistency'] = consistency
        
        # 3. å”¯ä¸€æ€§
        if 'posts' in df.columns:
            duplicates = df['posts'].duplicated().sum()
            uniqueness = 1 - (duplicates / len(df))
            quality_report['uniqueness'] = uniqueness
        
        # 4. æœ‰æ•ˆæ€§
        validity_issues = 0
        if 'posts' in df.columns:
            too_short = (df['posts'].str.len() < 10).sum()
            too_long = (df['posts'].str.len() > 50000).sum()
            validity_issues += too_short + too_long
        
        validity = 1 - (validity_issues / len(df))
        quality_report['validity'] = validity
        
        # æ€»ä½“è´¨é‡åˆ†æ•°
        quality_report['overall_quality'] = np.mean([
            completeness, consistency, uniqueness, validity
        ])
        
        return quality_report
    
    def clean_and_standardize_data(self, df):
        """æ¸…æ´—å’Œæ ‡å‡†åŒ–æ•°æ®"""
        print("æ‰§è¡Œæ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–...")
        
        cleaned_df = df.copy()
        cleaning_log = []
        
        # 1. å¤„ç†ç¼ºå¤±å€¼
        missing_posts = cleaned_df['posts'].isnull().sum()
        cleaned_df = cleaned_df.dropna(subset=['posts'])
        cleaning_log.append(f"åˆ é™¤postsç¼ºå¤±çš„è¡Œ: {missing_posts} è¡Œ")
        
        missing_types = cleaned_df['type'].isnull().sum()
        cleaned_df = cleaned_df.dropna(subset=['type'])
        cleaning_log.append(f"åˆ é™¤typeç¼ºå¤±çš„è¡Œ: {missing_types} è¡Œ")
        
        # 2. æ ‡å‡†åŒ–MBTIç±»å‹æ ¼å¼
        cleaned_df['type'] = cleaned_df['type'].str.upper()
        valid_types = ['INTJ', 'INTP', 'ENTJ', 'ENTP', 'INFJ', 'INFP', 'ENFJ', 'ENFP',
                      'ISTJ', 'ISFJ', 'ESTJ', 'ESFJ', 'ISTP', 'ISFP', 'ESTP', 'ESFP']
        
        invalid_count = (~cleaned_df['type'].isin(valid_types)).sum()
        cleaned_df = cleaned_df[cleaned_df['type'].isin(valid_types)]
        cleaning_log.append(f"åˆ é™¤æ— æ•ˆMBTIç±»å‹: {invalid_count} è¡Œ")
        
        # 3. æ¸…æ´—æ–‡æœ¬å†…å®¹
        original_count = len(cleaned_df)
        min_length = 20
        cleaned_df = cleaned_df[cleaned_df['posts'].str.len() >= min_length]
        cleaning_log.append(f"åˆ é™¤è¿‡çŸ­æ–‡æœ¬(<{min_length}å­—ç¬¦): {original_count - len(cleaned_df)} è¡Œ")
        
        max_length = 50000
        long_text_count = (cleaned_df['posts'].str.len() > max_length).sum()
        cleaned_df = cleaned_df[cleaned_df['posts'].str.len() <= max_length]
        cleaning_log.append(f"åˆ é™¤è¿‡é•¿æ–‡æœ¬(>{max_length}å­—ç¬¦): {long_text_count} è¡Œ")
        
        # 4. æ ‡å‡†åŒ–æ–‡æœ¬
        cleaned_df['normalized_posts'] = cleaned_df['posts'].apply(self.normalize_english_text)
        
        # 5. å»é™¤é‡å¤æ•°æ®
        duplicate_count = cleaned_df['normalized_posts'].duplicated().sum()
        cleaned_df = cleaned_df.drop_duplicates(subset=['normalized_posts'])
        cleaning_log.append(f"åˆ é™¤é‡å¤æ–‡æœ¬: {duplicate_count} è¡Œ")
        
        # 6. é‡ç½®ç´¢å¼•
        cleaned_df = cleaned_df.reset_index(drop=True)
        
        print(f"æ•°æ®æ¸…æ´—å®Œæˆ:")
        for log in cleaning_log:
            print(f"  - {log}")
        print(f"æœ€ç»ˆæ•°æ®é‡: {len(cleaned_df)} è¡Œ")
        
        return cleaned_df, cleaning_log
    
    def normalize_english_text(self, text):
        """æ ‡å‡†åŒ–è‹±æ–‡æ–‡æœ¬"""
        if not text or pd.isna(text):
            return ""
        
        text = str(text)
        
        # 1. è½¬æ¢ä¸ºå°å†™
        text = text.lower()
        
        # 2. å¤„ç†ç¼©å†™
        contractions = {
            "won't": "will not", "can't": "cannot", "n't": " not",
            "i'm": "i am", "you're": "you are", "it's": "it is",
            "he's": "he is", "she's": "she is", "we're": "we are",
            "they're": "they are", "i've": "i have", "you've": "you have",
            "we've": "we have", "they've": "they have", "i'll": "i will",
            "you'll": "you will", "he'll": "he will", "she'll": "she will",
            "we'll": "we will", "they'll": "they will", "i'd": "i would",
            "you'd": "you would", "he'd": "he would", "she'd": "she would",
            "we'd": "we would", "they'd": "they would"
        }
        
        for contraction, expansion in contractions.items():
            text = text.replace(contraction, expansion)
        
        # 3. ç§»é™¤URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' [URL] ', text)
        
        # 4. æ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™åŸºæœ¬æ ‡ç‚¹
        text = re.sub(r'[^\w\s\.\!\?\,\;\:]', ' ', text)
        
        # 5. å¤„ç†é‡å¤æ ‡ç‚¹
        text = re.sub(r'([.!?]){2,}', r'\1', text)
        
        # 6. æ¸…ç†å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def local_data_augmentation(self, df, augmentation_factor=1.5):
        """æœ¬åœ°æ•°æ®å¢å¼º"""
        print("æ‰§è¡Œæœ¬åœ°æ•°æ®å¢å¼º...")
        
        augmented_data = []
        original_data = df.to_dict('records')
        
        for record in original_data:
            # ä¿ç•™åŸå§‹æ•°æ®
            augmented_data.append(record)
            
            text = record['normalized_posts']
            
            # 1. åŒä¹‰è¯æ›¿æ¢
            if self.augmentation_methods['synonym_replacement']:
                aug_text = self.synonym_replacement(text, n=3)
                if aug_text != text:
                    aug_record = record.copy()
                    aug_record['posts'] = aug_text
                    aug_record['normalized_posts'] = aug_text
                    aug_record['augmentation_method'] = 'synonym_replacement'
                    augmented_data.append(aug_record)
            
            # 2. éšæœºæ’å…¥
            if self.augmentation_methods['random_insertion']:
                aug_text = self.random_insertion(text, n=2)
                if aug_text != text:
                    aug_record = record.copy()
                    aug_record['posts'] = aug_text
                    aug_record['normalized_posts'] = aug_text
                    aug_record['augmentation_method'] = 'random_insertion'
                    augmented_data.append(aug_record)
            
            # 3. éšæœºäº¤æ¢
            if self.augmentation_methods['random_swap']:
                aug_text = self.random_swap(text, n=2)
                if aug_text != text:
                    aug_record = record.copy()
                    aug_record['posts'] = aug_text
                    aug_record['normalized_posts'] = aug_text
                    aug_record['augmentation_method'] = 'random_swap'
                    augmented_data.append(aug_record)
            
            # 4. å¥å­é‡ç»„
            if self.augmentation_methods['sentence_reordering']:
                aug_text = self.sentence_reordering(text)
                if aug_text != text:
                    aug_record = record.copy()
                    aug_record['posts'] = aug_text
                    aug_record['normalized_posts'] = aug_text
                    aug_record['augmentation_method'] = 'sentence_reordering'
                    augmented_data.append(aug_record)
        
        # é™åˆ¶å¢å¼ºåçš„æ•°æ®é‡
        target_size = int(len(df) * augmentation_factor)
        augmented_df = pd.DataFrame(augmented_data)
        
        if len(augmented_df) > target_size:
            # ä¿ç•™æ‰€æœ‰åŸå§‹æ•°æ®ï¼Œéšæœºé‡‡æ ·å¢å¼ºæ•°æ®
            original_df = augmented_df[augmented_df.get('augmentation_method', 'original') == 'original']
            augmented_only = augmented_df[augmented_df.get('augmentation_method', 'original') != 'original']
            
            if len(augmented_only) > 0:
                sample_size = min(target_size - len(original_df), len(augmented_only))
                sampled_aug = augmented_only.sample(n=sample_size, random_state=42)
                final_df = pd.concat([original_df, sampled_aug], ignore_index=True)
            else:
                final_df = original_df
        else:
            final_df = augmented_df
        
        print(f"æœ¬åœ°æ•°æ®å¢å¼ºå®Œæˆ: {len(df)} -> {len(final_df)} æ ·æœ¬")
        return final_df.reset_index(drop=True)
    
    def synonym_replacement(self, text, n=1):
        """åŒä¹‰è¯æ›¿æ¢"""
        words = text.split()
        new_words = words.copy()
        
        # ç®€å•çš„åŒä¹‰è¯æ˜ å°„
        synonym_dict = {
            'good': ['great', 'excellent', 'wonderful', 'fantastic'],
            'bad': ['terrible', 'awful', 'horrible', 'poor'],
            'big': ['large', 'huge', 'enormous', 'massive'],
            'small': ['tiny', 'little', 'mini', 'compact'],
            'happy': ['joyful', 'cheerful', 'delighted', 'pleased'],
            'sad': ['unhappy', 'depressed', 'upset', 'disappointed'],
            'think': ['believe', 'consider', 'suppose', 'assume'],
            'like': ['enjoy', 'appreciate', 'prefer', 'favor'],
            'want': ['desire', 'wish', 'need', 'require'],
            'see': ['observe', 'notice', 'view', 'watch']
        }
        
        replacements_made = 0
        for i, word in enumerate(words):
            if replacements_made >= n:
                break
            word_lower = word.lower()
            if word_lower in synonym_dict and random.random() < 0.3:
                new_words[i] = random.choice(synonym_dict[word_lower])
                replacements_made += 1
        
        return ' '.join(new_words)
    
    def random_insertion(self, text, n=1):
        """éšæœºæ’å…¥"""
        words = text.split()
        
        # æ’å…¥çš„è¯æ±‡
        insert_words = ['really', 'very', 'quite', 'pretty', 'somewhat', 'rather']
        
        for _ in range(n):
            if len(words) > 0:
                random_idx = random.randint(0, len(words))
                random_word = random.choice(insert_words)
                words.insert(random_idx, random_word)
        
        return ' '.join(words)
    
    def random_swap(self, text, n=1):
        """éšæœºäº¤æ¢"""
        words = text.split()
        new_words = words.copy()
        
        for _ in range(n):
            if len(new_words) >= 2:
                idx1 = random.randint(0, len(new_words) - 1)
                idx2 = random.randint(0, len(new_words) - 1)
                new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]
        
        return ' '.join(new_words)
    
    def sentence_reordering(self, text):
        """å¥å­é‡ç»„"""
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        if len(sentences) > 2:
            random.shuffle(sentences)
            return '. '.join(sentences) + '.'
        return text
    
    def balance_dataset(self, df):
        """æ•°æ®å¹³è¡¡å¤„ç†"""
        print("æ‰§è¡Œæ•°æ®å¹³è¡¡å¤„ç†...")
        
        if 'type' not in df.columns:
            return df
        
        type_counts = df['type'].value_counts()
        target_count = int(type_counts.median())
        
        balanced_dfs = []
        
        for mbti_type in type_counts.index:
            type_data = df[df['type'] == mbti_type]
            current_count = len(type_data)
            
            if current_count > target_count:
                # ä¸‹é‡‡æ ·
                sampled_data = resample(type_data, n_samples=target_count, random_state=42)
                balanced_dfs.append(sampled_data)
            elif current_count < target_count:
                # ä¸Šé‡‡æ ·
                upsampled_data = resample(type_data, n_samples=target_count, random_state=42, replace=True)
                balanced_dfs.append(upsampled_data)
            else:
                balanced_dfs.append(type_data)
        
        balanced_df = pd.concat(balanced_dfs, ignore_index=True)
        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(f"æ•°æ®å¹³è¡¡å®Œæˆ: {len(df)} -> {len(balanced_df)} æ ·æœ¬")
        return balanced_df
    
    def extract_english_keywords(self, df):
        """æå–è‹±æ–‡å…³é”®è¯"""
        print("æ‰§è¡Œè‹±æ–‡å…³é”®è¯æå–...")
        
        if 'type' not in df.columns or 'normalized_posts' not in df.columns:
            return {}
        
        results = {}
        
        for dimension in ['E', 'S', 'T', 'J']:
            # ç¡®å®šæ­£è´Ÿæ ·æœ¬
            dimension_pos = {'E': 0, 'S': 1, 'T': 2, 'J': 3}[dimension]
            positive_samples = df[df['type'].str[dimension_pos] == dimension]
            negative_samples = df[df['type'].str[dimension_pos] != dimension]
            
            if len(positive_samples) == 0 or len(negative_samples) == 0:
                continue
            
            try:
                # TF-IDFåˆ†æ
                vectorizer = TfidfVectorizer(
                    max_features=1000,
                    stop_words=list(self.english_stopwords),
                    min_df=2,
                    max_df=0.8,
                    ngram_range=(1, 2)
                )
                
                pos_texts = positive_samples['normalized_posts'].tolist()
                neg_texts = negative_samples['normalized_posts'].tolist()
                all_texts = pos_texts + neg_texts
                
                tfidf_matrix = vectorizer.fit_transform(all_texts)
                feature_names = vectorizer.get_feature_names_out()
                
                # è®¡ç®—æ­£è´Ÿæ ·æœ¬çš„å¹³å‡TF-IDF
                pos_mean = tfidf_matrix[:len(pos_texts)].mean(axis=0).A1
                neg_mean = tfidf_matrix[len(pos_texts):].mean(axis=0).A1
                
                # è®¡ç®—å·®å¼‚åˆ†æ•°
                diff_scores = pos_mean - neg_mean
                
                # è·å–topå…³é”®è¯
                top_indices = np.argsort(np.abs(diff_scores))[-50:]
                dimension_keywords = []
                
                for idx in top_indices:
                    if abs(diff_scores[idx]) > 0.001:
                        keyword = feature_names[idx]
                        dimension_keywords.append({
                            'keyword': keyword,
                            'score': diff_scores[idx],
                            'positive_tfidf': pos_mean[idx],
                            'negative_tfidf': neg_mean[idx],
                            'abs_score': abs(diff_scores[idx]),
                            'is_predefined': keyword in self.mbti_keywords.get(dimension, set()) or 
                                           keyword in self.mbti_keywords.get({'E':'I', 'S':'N', 'T':'F', 'J':'P'}.get(dimension, ''), set())
                        })
                
                results[dimension] = sorted(dimension_keywords, key=lambda x: x['abs_score'], reverse=True)
                
            except Exception as e:
                print(f"å…³é”®è¯æå–å¤±è´¥ - {dimension}: {e}")
                continue
        
        return results
    
    def create_data_splits(self, df, test_size=0.2, val_size=0.1):
        """åˆ›å»ºæ•°æ®åˆ†å‰²"""
        print("åˆ›å»ºæ•°æ®åˆ†å‰²...")
        
        if 'type' not in df.columns:
            return {'train': df, 'val': None, 'test': None}
        
        # åˆ†å±‚åˆ†å‰²
        train_val, test = train_test_split(
            df, 
            test_size=test_size, 
            stratify=df['type'], 
            random_state=42
        )
        
        val_ratio = val_size / (1 - test_size)
        train, val = train_test_split(
            train_val,
            test_size=val_ratio,
            stratify=train_val['type'],
            random_state=42
        )
        
        splits = {
            'train': train.reset_index(drop=True),
            'val': val.reset_index(drop=True),
            'test': test.reset_index(drop=True)
        }
        
        print(f"æ•°æ®åˆ†å‰²å®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(splits['train'])} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(splits['val'])} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(splits['test'])} æ ·æœ¬")
        
        return splits
    
    def generate_comprehensive_report(self, original_df, final_df, keywords, output_dir):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print("ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        
        report = {
            'processing_summary': {
                'original_samples': len(original_df),
                'final_samples': len(final_df),
                'processing_timestamp': datetime.now().isoformat(),
                'processing_type': 'English_Only_Enhancement'
            },
            'original_analysis': self.comprehensive_data_analysis(original_df),
            'final_analysis': self.comprehensive_data_analysis(final_df),
            'keyword_analysis': {
                'total_dimensions': len(keywords),
                'keywords_per_dimension': {dim: len(kws) for dim, kws in keywords.items()},
                'top_keywords_summary': {}
            },
            'quality_improvements': {},
            'recommendations': []
        }
        
        # è®¡ç®—è´¨é‡æ”¹è¿›
        orig_quality = report['original_analysis']['quality_assessment']
        final_quality = report['final_analysis']['quality_assessment']
        
        for metric in orig_quality:
            if metric in final_quality:
                improvement = final_quality[metric] - orig_quality[metric]
                report['quality_improvements'][metric] = {
                    'original': orig_quality[metric],
                    'final': final_quality[metric],
                    'improvement': improvement,
                    'improvement_percentage': improvement / orig_quality[metric] * 100 if orig_quality[metric] > 0 else 0
                }
        
        # å…³é”®è¯æ‘˜è¦
        for dimension, keyword_list in keywords.items():
            if keyword_list:
                top_5 = keyword_list[:5]
                report['keyword_analysis']['top_keywords_summary'][dimension] = [
                    {'keyword': kw['keyword'], 'score': kw['score']} for kw in top_5
                ]
        
        # ç”Ÿæˆå»ºè®®
        report['recommendations'] = [
            "æ•°æ®å·²æ¸…æ´—å¹¶æ ‡å‡†åŒ–ï¼Œå¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒ",
            "å»ºè®®ä½¿ç”¨balancedæ•°æ®é›†è¿›è¡Œè®­ç»ƒä»¥è·å¾—æœ€ä½³æ€§èƒ½",
            "å…³é”®è¯å¯ç”¨äºç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹è§£é‡Š",
            "è€ƒè™‘ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½",
            "å¯æ ¹æ®keywordsæ„å»ºMBTIç‰¹å¾è¯å…¸"
        ]
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(output_dir, 'english_mbti_analysis_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report
    
    def process_english_enhancement(self, file_path):
        """å®Œæ•´çš„è‹±æ–‡æ•°æ®å¢å¼ºå¤„ç†æµç¨‹"""
        print("=== å¯åŠ¨è‹±æ–‡MBTIæ•°æ®é›†å¢å¼ºå¤„ç† ===")
        
        # 1. åŠ è½½åŸå§‹æ•°æ®
        original_df = pd.read_csv(file_path)
        print(f"åŸå§‹æ•°æ®åŠ è½½å®Œæˆ: {len(original_df)} æ ·æœ¬")
        
        # 2. æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
        cleaned_df, cleaning_log = self.clean_and_standardize_data(original_df)
        
        # 3. æœ¬åœ°æ•°æ®å¢å¼º
        augmented_df = self.local_data_augmentation(cleaned_df)
        
        # 4. æ•°æ®å¹³è¡¡å¤„ç†
        balanced_df = self.balance_dataset(augmented_df)
        
        # 5. è‹±æ–‡å…³é”®è¯æå–
        keywords = self.extract_english_keywords(balanced_df)
        
        # 6. åˆ›å»ºæ•°æ®åˆ†å‰²
        data_splits = self.create_data_splits(balanced_df)
        
        # 7. ä¿å­˜æ‰€æœ‰æ•°æ®é›†
        output_dir = os.path.dirname(file_path)
        
        datasets_to_save = {
            'cleaned': cleaned_df,
            'augmented': augmented_df,
            'balanced': balanced_df,
            'train': data_splits['train'],
            'val': data_splits['val'],
            'test': data_splits['test']
        }
        
        saved_files = {}
        for name, dataset in datasets_to_save.items():
            if dataset is not None:
                filename = f'enhanced_english_mbti_{name}.csv'
                filepath = os.path.join(output_dir, filename)
                dataset.to_csv(filepath, index=False)
                saved_files[name] = filepath
                print(f"å·²ä¿å­˜ {name} æ•°æ®é›†: {filepath}")
        
        # 8. ä¿å­˜å…³é”®è¯
        keywords_file = os.path.join(output_dir, 'english_keywords.json')
        with open(keywords_file, 'w', encoding='utf-8') as f:
            json.dump(keywords, f, indent=2, ensure_ascii=False, default=str)
        print(f"å…³é”®è¯å·²ä¿å­˜: {keywords_file}")
        
        # 9. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = self.generate_comprehensive_report(original_df, balanced_df, keywords, output_dir)
        
        print("=== è‹±æ–‡æ•°æ®å¢å¼ºå¤„ç†å®Œæˆ ===")
        
        return {
            'datasets': datasets_to_save,
            'saved_files': saved_files,
            'keywords': keywords,
            'data_splits': data_splits,
            'cleaning_log': cleaning_log,
            'analysis_report': report
        }

def main():
    file_path = r"C:\Users\lnasl\Desktop\DeepMBTI\data\Text\mbti_1.csv"
    
    if not os.path.exists(file_path):
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    # ä¸‹è½½å¿…è¦çš„NLTKèµ„æºï¼ˆå¦‚æœéœ€è¦ï¼‰
    try:
        import nltk
        nltk.download('punkt', quiet=True)
        nltk.download('stopwords', quiet=True)
        nltk.download('wordnet', quiet=True)
    except:
        print("NLTKèµ„æºä¸‹è½½å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®èµ„æº")
    
    processor = EnglishMBTIDatasetProcessor()
    results = processor.process_english_enhancement(file_path)
    
    print("\n=== è‹±æ–‡æ•°æ®é›†å¢å¼ºå¤„ç†æ€»ç»“ ===")
    print("âœ… æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–")
    print("âœ… æœ¬åœ°æ•°æ®å¢å¼ºï¼ˆåŒä¹‰è¯ã€é‡ç»„ç­‰ï¼‰")
    print("âœ… MBTIç±»å‹å¹³è¡¡å¤„ç†")
    print("âœ… è‹±æ–‡å…³é”®è¯æå–")
    print("âœ… æ•°æ®åˆ†å‰²ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼‰")
    print("âœ… è´¨é‡è¯„ä¼°å’Œæ”¹è¿›åˆ†æ")
    print("âœ… ç»¼åˆå¤„ç†æŠ¥å‘Š")
    
    print(f"\nç”Ÿæˆçš„æ•°æ®é›†æ–‡ä»¶:")
    for name, filepath in results['saved_files'].items():
        if name == 'balanced':
            print(f"  ğŸ¯ {name}: {filepath} (æ¨èç”¨äºè®­ç»ƒ)")
        else:
            print(f"  - {name}: {filepath}")
    
    print(f"\nå…³é”®è¯æ–‡ä»¶: {os.path.join(os.path.dirname(file_path), 'english_keywords.json')}")
    print(f"åˆ†ææŠ¥å‘Š: {os.path.join(os.path.dirname(file_path), 'english_mbti_analysis_report.json')}")
    
    # æ˜¾ç¤ºä¸€äº›å…³é”®ç»Ÿè®¡
    if 'keywords' in results:
        print(f"\nå…³é”®è¯æå–ç»“æœ:")
        for dim, keywords in results['keywords'].items():
            if keywords:
                top_3 = keywords[:3]
                print(f"  {dim}: {', '.join([kw['keyword'] for kw in top_3])}")

if __name__ == "__main__":
    main()